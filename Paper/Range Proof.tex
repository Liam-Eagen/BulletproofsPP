\documentclass[10pt,oneside,a4paper]{article}

\usepackage{geometry}
\geometry{margin=1in}
\usepackage{amsthm,amsmath,amsfonts}

\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\norm}[1]{\left| #1 \right|^2}					% \abs{#1}^2
\newcommand{\ip}[2]{\left\langle #1, #2 \right\rangle}
%\newcommand{\ip}[2]{#1 #2}
%\newcommand{\com}[2]{\text{Com}_{#1}\left(#2\right)}
\newcommand{\com}[2]{\text{Com}\left(#2\right)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\len}[1]{\text{len}\left( #1 \right)}

\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}

\title{Bulletproofs++}

\author{Liam Eagen \\ liameagen@protonmail.com}
%\date{2020}

\begin{document}

\maketitle

%\begin{abstract}
%This paper is organized into two sections. First, I describe a probabilistic argument to efficiently show that some committed values belong to a set of possible choices. I discuss several examples where such an argument is useful, in particular for constructing range proofs using non-binary digits. In the second part, I describe a modification to the Bulletproof and Bulletproof+ style inner product argument to a norm squared, i.e. self inner product, argument. This can be used to further reduce the size of a binary range proof, as compared to Bulletproofs+, as well as reduce the length of the vector commitments by half. Using a reciprocal argument, the length of these commitments can be reduced even further, e.g. a factor $8$ in the case of $64$ bit range proofs common in practice with size equal to Bulletproofs+. I also describe a modification to range proofs generally, which is applicable to Bulletproofs(+) in all likelihood, to support arbitrary integer ranges and apply this to all of the binary norm, arbitrary inner product, and arbitrary norm range proofs.
%\end{abstract}

\begin{abstract}
Building on Bulletproofs \cite{} and Bulletproofs+ \cite{}, I describe several new range proofs that achieve both shorter proof sizes and witness lengths as well as a confidential transaction protocol for multiple types. To do this, first I modify the inner product argument to show that the norm squared of a given vector is equal to a value in zero knowledge, which yields a binary digit range proof of half the witness length. Second, I use a new permutation argument which is essentially the logarithmic derivative of \cite{} to construct range proofs for arbitrary bases, which reduces witness length by an a factor of between $6$ and $8$ in the case of $64$ bit ranges and base $16$. Finally, I use the permutation argument to show that conservation of money holds for a collection of typed values in zero knowledge with respect to the particular types and the relationships of types of distinct values. 
\end{abstract}

\section{Introduction}

Zero knowledge proof research has experienced a Cambrian explosion in recent years, due in large part to the substantial increase in attention by cryptocurrencies. Zero knowledge proofs offer numerous tradeoffs between various properties including power, security assumptions, trusted setups, proof size, prover and verifier time, etc. Among these proof systems, Bulletproofs \cite{} are situated at a uniquely attractive place in property space as they can be made non-interactive via Fiat-Shamir, do not require additional security assumptions beyond those necessary for elliptic curve based signature schemes, do not require a trusted setup, and have relatively small proofs. Unfortunately, they have linear verification time in the witness, which limits their utility as a general purpose zero knowledge proof system like SNARKs. They have practical applications for small proofs, specifically for range proofs in cryptocurrencies like Monero, MimbleWimble based currencies like Grin, and other confidential transaction protocols without a trusted setup.

Bulletproofs use a recursive structure to prove that a Pedersen commitment to a pair of vectors is equal to a committed scalar. At each step in the proof, the prover is able to transform the committed vector into a vector of half the length using a constant amount of communication with the verifier, resulting in an overall proof size logarithmic in the length of witness. This builds on the earlier work of Bootle et al \cite{} reducing the proof size by a factor of three and accepting inputs via Pedersen commitments.

Bulletproofs are further improved upon in Bulletproofs+ \cite{} to support a weighted inner product structure. This allows the prover to efficiently prove knowledge of the sum $\sum_{i=0}^{2n - 1} x_i y_i q^i$ by breaking it down into two weighted sums of half the size

$$ \sum_{i=0}^{2n - 1} x_i y_i q^i = \sum_{i=0}^{n - 1} x_i y_i q^i + q^n \sum_{i=0}^{n-1} x_{i + n} y_{i+n} q^i $$

This can be generalized to any vector that can be written as a tensor product of dimension two vectors, and I will make use of essentially the same construction in Bulletproof++.

I continue to build on this by using modifying the recursive structure of the Bulletproof inner product argument to show that a committed vector's self inner product is equal to a committed scalar while only committing to the vector once. This immediately yields a smaller binary digit range proof, as the inner product based range proofs require committing to both $d$ and $d - 1$ for each digit but using a self inner product this can be reduced to one committed value per digit. 

I then introduce a modification of the permutation argument in \cite{} that allows efficiently constructing range proofs using larger bases. Essentially, rather than constructing two polynomials to show that two sets are equal, we can construct one polynomial that encodes the digit symbols of the base $b$ with multiplicity and another with the multiplicities secret but the bases public. Taking the logarithmic derivative of both sides transforms the digit polynomial into a sum of reciprocals of monomials and puts the multiplicities as the numerators of the reciprocals with the denominator public.

This technique turns out to be quite powerful, and I also use to build a more complex confidential transaction protocol for multiple types of currency. In that case, the denominators are monomials with roots at values corresponding to types of currency and the numerators are the amount of currency. This allows a multitype conservation of money protocol that is zero knowledge in both the particular types of currency involved and the relationships the types of various inputs and outputs. In cryptocurrencies that do not hide transaction metadata, this degree of zero knowledge is basically required to keep transaction input and output types confidential, since otherwise an observer could track type information 

\section{Preliminaries}

T

\subsection{Notation}

We will assume the existence of a prime order group with order $p$, dependent on the security parameter. The elements of this group will be denoted by capital letters and may be referred to as either group elements or curve points, since this group will typically be instantiated to a prime order subgroup of an elliptic curve. Field elements in the field of characteristic $p$, $\mathbb{F}_p$, will be referred to with lowercase letters. I will use additive notation for the group and bold letters for vectors of either group elements or field elements, which will be zero indexed. The inner product of vectors will be written as $\ip{\vec{x}}{\vec{G}} = \sum_i x_i G_i$. When necessary vectors are assumed to be padded with zeros on the right, i.e. higher order indices. 

In the inner product and norm arguments, we need to split a sequence of vectors into nearly equal halves. For a vector $\vec{v}$ the halves are denoted $\vec{v}_0$ and $\vec{v}_1$ such that

$$ \len{\vec{v}_0} = \ceil{\frac{\len{\vec{v}}}{2}}
   \text{ and } \len{\vec{v}_1} = \floor{\frac{\len{\vec{v}}}{2}}$$

In the Bulletproofs and Bulletproofs+ these halves are explicitly defined so that concatenating $\vec{v}_0$ and $\vec{v}_1$ yields $\vec{v}$, but this is not actually necessary for correctness. In Bulletproofs++ I will define the halves to be interleaved, so that $\vec{v}_0$ is the vector of even elements of $\vec{v}$ and $\vec{v}_1$ is the vector of odd elements. This makes a minor difference in the weighted inner product and weighted norm arguments. Note that if we assume $\vec{v}$ has a power of two length, these two subvectors will always have equal size and that padding them with zeros as necessary is equivalent.

Matrices will also be denoted by capital letters. In particular, the matrix $D(x)$ is the diagonal matrix of powers of $x$ such that $D(x)_{i i} = x^i$ using zero based indexing, and the matrix $Q = q D(q)$ which starts at the first power of $q$, which can be inferred from context. It will generally be clear from context whether I am referring to a group element or a matrix as matrices almost always appear multiplying a vector. I will refer to the tensor product of two vectors, defined so

$$\vec{a} \otimes (b_0, b_1, ...) = b_0 \vec{a} \oplus b_1 \vec{a} \oplus ...$$ 

Commitments should be understood to be Pedersen commitments, which are perfectly hiding and computationally binding up to the discrete log problem. I will use the function $\com{}{v; \vec{y}, \vec{H}; \vec{x}, \vec{G}}$ to denote a commitment, which is defined for some implicit $s$ as

$$\com{}{v; \vec{y}, \vec{H}; \vec{x}, \vec{G}} = s H + v G + \ip{\vec{y}}{\vec{H}} + \ip{\vec{x}}{\vec{G}}$$

Note that the commitment function is linear in all of its scalar and group element argument. From context, $s$ will either be randomly sampled or entirely determined as a linear combination of blinding values from other commitments.

\subsection{Cryptography}

\textbf{Note: incomplete}

The cryptographic framework for Bulletproof++ protocols is the same as that of Bulletproofs and Bulletproofs+. We assume that the discrete log problem in the prime order group is hard, with work exponential in some security parameter $\lambda$ necessary to compute the discrete log. All protocols are defined without a trusted set up and can be made non-interactive in the random oracle model. In the protocols, every commitment should be understood to be appended to a transcript and every challenge value generated by hashing the transcript with a hash function modeled as a random oracle.

The three essential properties of all the protocols that we are concerned with are: completeness, Special Honest Verifier Zero Knowledge (SHVZK), and Computational Witness Extended Emulation (CWE). Informally, completeness means that any proof generated from a valid witness will be accepted by the verifier and any proof generated from an invalid witness will not be accepted by the verifier, SHVZK means that the verifier learns nothing about the witness from the proof, and CWE means that the prover must know a valid witness to construct a proof. 

Concretely, completeness usually follows from many applications of Schwartz-Zippel lemma, which states that a multivariate polynomial of total degree $d$ over a field evaluated at a vector of uniformly randomly points chosen from $N$ values each equals zero but is not identically zero with probability $d / N$. SHVZK and CWE are shown by manipulating the random oracle to either produce an identically distributed transcript without using the witness, SHVZK, or to extract the witness by evaluating at multiple challenges, CWE. The latter uses the forking lemma, similarly to the other Bulletproof protocols.

\section{Norm Argument}

Bulletproofs use a recursive technique to transform a commitment to two vectors and their inner product

$$ C = \com{}{\ip{\vec{x}}{\vec{y}}; \vec{x}, \vec{G}; \vec{y}, \vec{H}} $$

Into a commitment $C' = \com{}{\ip{\vec{x}'}{\vec{y}'}; \vec{x}', \vec{G}'; \vec{y}', \vec{H}'}$ to two vectors of half the length such that if $C'$ commits to the inner product of $\vec{x}'$ and $\vec{y}'$, it follows with overwhelming probability that $C$ commits to the inner product of $\vec{x}$ and $\vec{y}$. The proof structure, as well as completeness, follows from the following identity of polynomials in $e$ evaluated at a random point of the verifier's choosing and the Schwartz-Zippel lemma

$$ \ip{\frac{1}{e} \vec{x}_0 + e \vec{x}_1}{e \vec{y}_0 + \frac{1}{e} \vec{y}_1} =
   \frac{1}{e^2} \ip{\vec{x}_0}{\vec{y}_1} +
   \ip{\vec{x}}{\vec{y}}
   + e^2 \ip{\vec{x}_1}{\vec{y}_0} $$ 

Applying this identity to all three inner products in the commitment, $\ip{\vec{x}}{\vec{y}}$, $\ip{\vec{x}}{\vec{G}}$, and $\ip{\vec{y}}{\vec{H}}$, the prover can send only two additional commitments $L$ and $R$ that enable the verifier to evaluate the identity for all the inner products at a random point.

$$ L = \com{}{\ip{\vec{x}_0}{\vec{y}_1}; \vec{x}_0, \vec{G}_1 ; \vec{y}_1, \vec{H}_0} $$
$$ R = \com{}{\ip{\vec{x}_1}{\vec{y}_0}; \vec{x}_1, \vec{G}_0 ; \vec{y}_0, \vec{H}_1} $$

Using these values, the verifier can select a random point $e$ to construct the new vectors

$$ \vec{G}' = e \vec{G}_0 + \frac{1}{e} \vec{G}_1 \qquad \vec{H}' = \frac{1}{e} \vec{H}_0 + e \vec{H}_1 $$
$$ \vec{x}' = \frac{1}{e} \vec{x}_0 + e \vec{x}_1 \qquad \vec{y}' = e \vec{y}_0 + \frac{1}{e} \vec{y}_1 $$

So that

$$ C' = C + \frac{1}{e^2} L + e^2 R = \com{}{\ip{\vec{x}'}{\vec{y}'} ; \vec{x}', \vec{G}' ; \vec{y}', \vec{H}'} $$

This argument can be applied recursively to generate a sequence of responses $(L_i, R_i)$ and commitments $C_i$ to vectors of the of sizes $n_i = \ceil{n_{i-1}/2} $ such that $C_{i-1} = C_i + 1/e_i^2 L_i + e_i^2 R_i$. Eventually, the prover will be left with the commitment $C_0 = s_0 H + v G + x G' + y H'$ which cannot be further reduced. At this point, the prover can open the commitment to show that the final scalars satisfy the one dimensional inner product relation, i.e. $v = x y$, or the prover can show the relation is satisfied in zero knowledge. To verify, the verifier will take the sequence of $(L_i, R_i)$, use the challenges to compute $H'$ and $G'$, and verify that

$$C + \sum_{i=0}^k \frac{1}{e_i^2} L_i + e_i^2 R_i = s_0 H + v G + x G' + y H' \text{ and } x y = v$$

\subsection{Weighted Inner Product}

This technique is modified in Bulletproofs+ to evaluate the inner product $\ip{Q \vec{x}}{\vec{y}}$ directly, which Bulletproofs do by separately scaling the vector and then using the inner product argument. This works by decomposing the $Q$ power matrix as a tensor product and combining the $q$ scaling into the inner product relation. In Bulletproofs+ this is done ``top down" starting with the largest $q^{2^i}$ and working down, since the subvectors concatenate to form the larger vector, but in Bulletproofs++ I do this ``bottom up" starting with the smallest $q^{2^i}$ and working up since the vectors are interleaved. Writing the weighted inner product with interleaved vectors we have

$$\vec{x}' = Q \vec{x} \qquad \vec{G}' = Q^{-1} \vec{G}$$
% $$\frac{1}{e} \vec{x}'_0 + e \vec{x}'_{1} = (\frac{1}{e} q D(q^2) \vec{x}_{0} + e q^2 D(q^2) \vec{x}_{1}) = q D(q^2) (\frac{1}{e}\vec{x}_{0} + e q \vec{x}_{1})$$
$$\ip{\frac{1}{e} \vec{x}'_0 + e \vec{x}'_1}{e \vec{y}_0 + \frac{1}{e} \vec{y}_1} = \frac{q}{e^2} \ip{D(q^2) \vec{x}_0}{\vec{y}_1} + \ip{Q \vec{x}}{\vec{y}} + q^2 e^2 \ip{D(q^2) \vec{x}_1}{\vec{y}_0}$$
$$\ip{\frac{1}{e} \vec{x}'_0 + e \vec{x}'_1}{e \vec{G}'_{0} + \frac{1}{e} \vec{G}'_{1}} = \frac{1}{q e^2} \ip{\vec{x}_0}{\vec{G}_1} + \ip{\vec{x}}{\vec{G}} + e^2 q \ip{\vec{x}_1}{\vec{G}_0}$$

Notice these are not the same, since the basis elements are also inversely scaled, while the vector $\vec{y}$ is not scaled at all. By appropriately incorporating the $q$ scaling into the responses $L$ and $R$, this can be straightforwardly transformed into a recursive argument reducing both $\vec{x}$ and $\vec{y}$ by half each round. 

\subsection{Norm Argument}

Obviously, the inner product argument requires committing to both the vectors $\vec{x}$ and $\vec{y}$. In the case that these vectors are actually the same, or even if they satisfy an affine relation, this is redundant. Trying to modify the inner product argument to work around this redundancy, we are obstructed by the asymmetric treatment of $\vec{x}$ and $\vec{y}$. That is, the first half of $\vec{x}$ is multiplied by $1/e$ but the first half of $\vec{y}$ is multiplied by $e$ and conversly for the second halves. We can modify the polynomial relation to treat both vectors symmetrically as follows

$$ \ip{\vec{x}_0 + e \vec{x}_1}{\vec{y}_0 + e \vec{y}_1} = 
   \ip{\vec{x}}{\vec{y}} +
   e \left( \ip{\vec{x}_0}{\vec{y}_1} + \ip{\vec{x}_1}{\vec{y}_0} \right) +
   (e^2-1) \ip{\vec{x}_1}{\vec{y}_1} $$

To transform this into a norm squared argument, we group terms from the inner products by coefficient, as in Bulletproofs, allowing the verifier to choose a challenge and evaluate the polynomial. Since the coefficients $1$, $e$, and $e^2 - 1$ are linearly independent over $\mathbb{F}_p$, this transformation is sound. Given the commitment

$$ C_n = \com{}{\norm{\vec{x}}; \vec{x}, \vec{G}} $$

The prover will commit to

$$X_n = \com{}{2 \ip{\vec{x}_0}{\vec{x}_1}; \vec{x}_0, \vec{G}_1; \vec{x}_1, \vec{G}_0} \qquad R_n = \com{}{\norm{\vec{x}_1}; \vec{x}_1, \vec{G}_1}$$

Which the verifier will use to compute the next commitment as 

$$ \vec{G}' = \vec{G}_0 + e \vec{G}_1 \qquad \vec{x}' = \vec{x}_0 + e \vec{x}_1 $$
$$ C_{n-1} = C_n + e X_n + (e^2-1) R_n = \com{}{\norm{\vec{x}'}; \vec{x}', \vec{G}'} $$

After recursively applying this argument, the protocol will terminate with a commitment $C_0$ to a value and its square. However, it is actually more efficient to stop the protocol at $C_1$ or $C_2$ as the final round replaces one scalar value in the opening with two group elements, and the second to last round replaces one or two scalars with two group elements. 

\subsubsection{Combined Linear Argument}

The new polynomial relation can be applied to other arguments besides a norm argument with the same challenge response structure. In particular, if one of the vectors is public, the prover can show that the other committed vector satisfies a linear relation using the same commitment structure. This allows the prover to simultaneously prove that a linear combination of the elements of one vector plus the norm of another equals another committed value.

This separate linear argument is useful in several situations. Firstly, when conducting a multiparty computation (MPC), sometimes users want to show that the sum of a collection of vectors satisfies some linear relation. Trying use the norm argument for this sort of proof either requires committing to each vector on distinct basis elements or evaluating the product of secret values, which is substantially more expensive. The linear argument avoids this problem entirely. Similarly, the linear relation is useful in avoiding extraneous commitments to all the coefficients of a polynomial where only one encodes the desired constraint.

For a public vector $\vec{c}$, committed vectors $\vec{l}$ and $\vec{n}$, and $v = \ip{\vec{c}}{\vec{l}} + \norm{\vec{n}}$ the prover will initially commit to

$$ C = \com{}{v; \vec{l}, \vec{H} ; \vec{n}, \vec{G}}$$

And then commit to the modified responses     

$$ X = \com{}{\ip{\vec{c}_0}{\vec{l}_1} + \ip{\vec{c}_1}{\vec{l}_0} + 2 \ip{\vec{n}_0}{\vec{n}_1}; 
     \vec{l}_0, \vec{H}_1; \vec{l}_1, \vec{H}_0 ; \vec{n}_0, \vec{G}_1 ; \vec{n}_1, \vec{G}_0} $$
$$ R = \com{}{\ip{\vec{c}_1}{\vec{l}_1} + \norm{\vec{n}_1} ; \vec{l}_1, \vec{H}_1; \vec{n}_1, \vec{G}_1} $$

The verifier will choose the challenge value and the prover will compute the next commitment and updated vectors as

$$ C' = C + e X + (e^2 - 1) R $$
$$ \vec{G}' = \vec{G}_0 + e \vec{G}_1 \qquad \vec{H}' = \vec{H}_0 + e \vec{H}_1 $$
$$ \vec{c}' = \vec{c}_0 + e \vec{c}_1 \qquad \vec{l}' = \vec{l}_0 + e \vec{l}_1 \qquad \vec{n}' = \vec{n}_0 + e \vec{n}_1 $$

In general $\vec{l}$ and $\vec{n}$ will not be the same length and the optimal number of rounds for overall proof size will depend on how much larger one is than the other. The optimal number of rounds $k$ can be defined such that increasing the number of rounds removes two or fewer total scalars from the final opening. This only becomes an issue when the lengths of the vectors are similar in size and one occurs in the ``upper half" of the range given by its number of bits. Stated precisely the optimal number of rounds $k$ is 

$$ r(\vec{v}) = \ceil{ \log_2 \text{len}(\vec{v}) } - 2 $$
$$ k = \begin{cases}
    \max(r(\vec{l}), r(\vec{n})) + 1 & \mbox{ if } \abs{k_l - k_n} \leq 1
                                       \mbox{ and } \max \left(\frac{\text{len}(\vec{l})}{2^{r(\vec{l})}}, \frac{\text{len}(\vec{n})}{2^{r(\vec{n})}} \right) = 4 \\
    \max(r(\vec{l}), r(\vec{n}))     & \mbox{ otherwise}
\end{cases} $$

\subsubsection{Fast Scalar Multiplication}

In proving the norm and linear arguments, as well as the original Bulletproof inner product argument, the prover must compute the updated basis points $\vec{G}'$ at each round of the protocol. This turns out to very expensive, even compared to the elliptic curve inner product computations, since the updating the basis does not benefit from optimizations like Shamir's trick.

We can substantially reduce the amount of time spent on computing the updated bases by taking advantage of the fact that each updated basis element is the same linear combination of the different basis points. That is, the updated basis points are

$$ \vec{G}' = \vec{G}_0 + e \vec{G}_1 $$

Given the challenge value, the prover can compute a small rational representation of $e$ modulo the order of the curve $p$. That is, a pair of small values $a$ and $b$, up to sign, such that $a - e b = 0 \mod p$. Such a solution can be computed via the extended Euclidean algorithm or more generally via a lattice reduction algorithm like LLL by finding a short vector in the lattice 

$$ L = \left\{ \left( a, b, q \right) : a - e b + p q = 0 \right\} $$ 

Given the rational representation, the prover can compute

$$ b \vec{G}' = b \vec{G}_0 + a \vec{G}_1 $$

These scalars will be about half the length of the field, so the right hand side can be evaluated in approximately half the number of elliptic curve operations using Shamir's trick. The left hand side multiplication can be deferred by keeping track of a single normalization factor, as this factor is the same for all the new basis elements. For details see the protocol in the next section which incorporates this optimization. For the original Bulletproof inner product argument we can apply a similar optimization to transform

$$ \vec{G}' = \frac{1}{e} \vec{G}_0 + e \vec{G}_1 $$

Using $a - e^2 b = 0 \mod p$ into

$$ e b \vec{G}' = b \vec{G}_0 + a \vec{G}_1 $$

In elliptic curves with complex multiplication, we can do even better by exploiting the fact that for some quadratic integer $\alpha$ computing $\alpha P$ is very efficient. This value of $\alpha$ allows using a larger lattice and finding an even smaller solution in

$$ \left\{ \left( a,b,c,d, q \right) : a + b \alpha - e c - e d \alpha + p q = 0 \right\} $$ 

Which is the set of solutions to

$$ e = \frac{a + b \alpha}{c + d \alpha} \mod p$$

These scalars will each be about one fourth the length of the field length and transform the basis update into a linear combination of four curve points. Depending on the specific inner product algorithm used and type of complex multiplication this may use nearly four times fewer elliptic curve operations than the original basis update. The lattice reduction will likely be more expensive than the simple rational representation case, so practical implementation may only benefit from this version for larger vectors. It may also be possible to sample directly $(a, b, c, d)$ more efficiently to generate $e$, although I will not explore this further in this paper.

\subsubsection{$q$-Power Weighted Norm}

To scale the norm by $q$ powers, we can use pretty much the same technique as the Bulletproof+ weighted inner products outlined earlier applied to the norm argument. Given the challenge value $q$ and the vectors $\vec{x}$ and $\vec{G}$ in the norm argument, the weighted norm is generated from the equations

$$\vec{x}'_0 + e \vec{x}'_{1} = (q D(q^2) \vec{x}_{0} + e q^2 D(q^2) \vec{x}_{1}) = q D(q^2) (\vec{x}_{0} + e q \vec{x}_{1})$$
$$\vec{G}'_{0} + e \vec{G}'_{1} = ((q D(q^2))^{-1} \vec{G}_{0} + e (q^2 D(q^2))^{-1} \vec{x}_{1}) = (q D(q^2))^{-1} (\vec{G}_{0} + \frac{e}{q} \vec{G}_{1}) $$
$$\ip{\vec{x}'_0 + e \vec{x}'_1}{\vec{G}'_0 + e \vec{G}'_1} = \ip{\vec{x}}{\vec{G}} + e \left( q^{-1} \ip{\vec{x}_0}{\vec{G}_1} + q \ip{\vec{x}_1}{\vec{G}_0} \right) + (e^2 - 1) \ip{\vec{x}_1}{\vec{G}_1}$$

Apart from modifying the scalars of $X$ and $R$ to account for the $q$ powers, the rest of the protocol works out mostly the same. Scaling by $q$ powers is also compatible with the fast basis update operations from the previous section by finding a short rational representation for $e/q$ for the norm basis updates.

\subsection{Complete Norm Linear $q$-Power Argument}

To integrate the fast scalar multiplications and the q power scaling into the prover, each round of the norm linear argument will accept four normalization factors $(s_s, s_l, s_n, q)$ where $q$ is not, except for the first round, the same as the initial q input. As described earlier, these normalizations allow for shorter basis update computations without affecting the security properties of the protocol. At the beginning, and end of each round, we have a commitment

$$\vec{C} = s H + v G + \ip{\vec{l}}{\vec{H}} + \ip{\vec{n}}{\vec{G}}$$

Such that the normalization factors at that point satisfy

$$v = s_s \ip{\vec{c}}{\vec{l}} + s_s s_n^2 \norm{D(q) \vec{n}} $$ 

And that scaling the vectors as $s_l \vec{l}$ and $s_n D(q) \vec{n}$ and the basis vectors inversely yields the unnormalized vectors at the same point in the protocol.

%TODO algorithm

\begin{enumerate}

\item Input $(\vec{G}, \vec{H}, \vec{c}, s_s, s_l, s_n, q, C = \com{}{v; \vec{l}, \vec{H}; \vec{n}, \vec{G}})$
\item Where $v_l = \ip{\vec{c}}{\vec{l}}$, $v_n = \norm{D(q) \vec{n}}$, and $v = s_s s_l v_l + s_s s_n^2 v_n$
\item Let $x = s_s s_l (\ip{\vec{c}_0}{\vec{l}_1} + \ip{\vec{c}_1}{\vec{l}_0}) + 2 s_s s_n^2 q \ip{D(q^2) \vec{x}_0}{D(q^2) \vec{x}_1}$ 
\item Let $r = s_s s_l \ip{\vec{c}_1}{\vec{l}_1} + s_s s_n^2 q^2 \norm{D(q^2) \vec{x}_1}$
\item Commit $X = \com{}{x; \vec{l}_1, \vec{H}_0; \vec{l}_0, \vec{H}_1; q \vec{n}_1, \vec{G}_0; q^{-1} \vec{n}_0, \vec{G}_1}$
\item Commit $R = \com{}{r; \vec{l}_1, \vec{H}_1; \vec{n}_1, \vec{G}_1}$
\item $e \leftarrow \mathbb{F}_p$
\item Find small $b_l e - a_l = 0$ and $b_n e - a_n q = 0 \mod p$
\item Let $q' = q^2$, $s'_l = b_l s_l$, and $s'_n = b_n s_n$ 
\item Let $\vec{c}' = b_l \vec{c}_0 + a_l \vec{c}_1$, $\vec{l}' = \frac{1}{b_l} \vec{l}_0 + \frac{e}{b_l} \vec{l}_1$, and $\vec{n}' = \frac{1}{b_n} \vec{n}_0 + \frac{e q}{b_n} \vec{n}_1$
\item Let $v' = v + e x + (e^2 - 1) r = s_s s_l \ip{\vec{c}'}{\vec{l}'} + s_s s_n^2 \norm{D(q') \vec{n}'}$

\item Let $\vec{H}' = b_l \vec{H}_0 + a_l \vec{H}_1$ and $\vec{G}' = b_n \vec{G}_0 + a_n \vec{G}_1$
\item Let $C' = C + e X + (e^2 - 1) R = \com{}{v'; \vec{l}', \vec{H}'; \vec{n}', \vec{G}'}$
\item Output $(\vec{G}', \vec{H}', \vec{c}', s_s, s'_l, s'_n, q', C')$
\end{enumerate}

For the first round, the prover will initialize $s_l = 1$ and $s_n = q$, which accounts for the $q$ powers starting at $1$ rather than zero. At the final round of the protocol, the prover will output the final $q$ value $q_f$ and theremaining vectors scaled by their respective normalization factors, $\vec{l}_f = s_l \vec{l}$ and $\vec{n}_f = s_n \vec{n}$. For verification, the verifier will receive the initial commitment $C$, the responses $(X_i, R_i)$, the final scalar $v$, and the final vectors $\vec{l}_f$ and $\vec{n}_f$. The verifier will expand out the challenges into two vectors

$$ \vec{e}_l = \bigotimes_{i=0}^{k} (1 \oplus e_i) \qquad \vec{e}_n = q^{-1} \bigotimes_{i=0}^{k} (1 \oplus \frac{e_i}{q^{2^i}})$$

And verification will consist of one field computation and one elliptic curve inner product. Keeping in mind that the tensor products may be longer than the basis vectors and that vectors should be padded with identity elements as necessary, these are

% TODO need to contract c?
$$ v = s_s \ip{\vec{c}}{\vec{e}_l \otimes \vec{l}_f} + s_s \norm{D(q_f) \vec{n}_f} $$
$$ C + \sum_{i=0}^k e_i X_i + (e_i^2 - 1) R_i = s H + v G + \ip{\vec{e}_l \otimes \vec{l}_f}{\vec{H}}
     + \ip{\vec{e}_n \otimes \vec{n}_f}{\vec{G}} $$

As I have already mentioned, it is possible to modify this protocol to break the vectors into concatenated, rather than interleaved, halves. To do this, the prover will traverse the $q^{2^i}$ in reverse order and the verifier will reverse the sequence of challenges in the tensor product. That is, the right most factor will be $e_0$ and the left most will be $e_k$, but the $q$ powers still increase left to right. Otherwise, the protocol is the same.

% TODO binary range proof here?
\section{Binary Range Proof}

Using the $q$ power norm argument, we can construct a binary digit range proof with witness half the size of an equivalent Bulletproof(+) range proof. In the inner product argument based protocols, to show that each committed digit $d_i$ actually is a bit, the prover commits to $d_i$ in one vector and the values $d_i - 1$ in the other and shows that their product is zero. If $(d_i - 1) d_i = 0$ it follows that $d_i \in \{0, 1\}$ and therefore that each digit really is a valid binary digit. This technique results in a total witness length that is twice the number of digits as the range. 

We can transform this bit constraint into one suitable for the norm argument by completing the square of the constraint polynomial. That is, by showing that

$$ \left( d_i - \frac{1}{2} \right)^2 - \frac{1}{4} = 0 $$

This replaces the multiplication with a square. To complete the range proof, we also need to verify the sum of the binary digits weighted by powers of $2$ is equal to the committed value. For the sake of generality, we can make a small modification to the range proof so that we can support arbitrary ranges. To show that $A \leq v < B$ for any integers $A < B$ such that $B - A < p$, we will instead show that $0 \leq v - A < B - A$ by construct the base vector $\vec{b}$ as

$$ n = \ceil{\log_2(B - A)} \qquad b_i = 2^i \text{ for } i = 0..n-2 \qquad b_{n-1} = (B - A) - 2^{n-1} $$

To see why this is valid, it suffices to check that the minimum binary linear combination of this base vector is $0$ and the maximum is

$$ b_{n-1} + \sum_{i=0}^{n-2} b_{n-2} = (B - A - 2^{n-1}) + (2^{n-1} - 1) = B - A - 1 $$

This is possible because certain elements of the ranges where $B - A$ is not a power of two are representable using multiple bit patterns. Given the base vector and after committing to the value $v$ and the digit vector $\vec{d}$, the verifier will select a challenge value $q$ and the prover will show that

% NOTE primes to start at one
\begin{align*}
    \norm{ Q \left(\vec{d} - \frac{1}{2} \vec{1} \right) + Q^{-1} \vec{b} } 
        &= \norm{Q \left(\vec{d} - \frac{1}{2} \vec{1} \right)} + 2 v - \ip{\vec{1}}{\vec{b}} + \norm{Q^{-1} \vec{b}} \\ 
        &= 2 v + \norm{\frac{1}{2} Q \vec{1} - Q^{-1} \vec{b}}
\end{align*}

The final norm term is independent of any committed values, and the equality holds only if the digits are valid and $\ip{\vec{b}}{\vec{d}} = v$. To extend this to multiple values, we can concatenate the digit and base vectors for each value and define a new diagonal matrix $U(x)$ which scales the digits of value $i$ by $x^{i+1}$. Scaling the base vector by this matrix we find

$$ \ip{U(x) \vec{b}}{\vec{d}} = \sum_{i = 0}^n x^{i+1} v_i $$

We can either sample a new challenge value $x$ for this or use odd $q$ powers. Finally, we must blind the digit vector. As discussed in the previous section, blinding can occur at any point before, during, or after the norm argument depending on how the range proof is used. In this case, I will describe how to blind before engaging the norm argument. Let the public values be

$$ P(x, q) = \left( \norm{\frac{1}{2} Q \vec{1} - Q^{-1} \vec{b}} - \sum_{i=0}^n x^{i+1} A \right) G + \ip{-\frac{1}{2} \vec{1} + U(x) Q^{-2}\vec{b}}{\vec{G}} $$

The binary range proof is then

\begin{enumerate}
    \item Input $C_i = \com{}{v_i}$
    \item Commit $D = \com{}{\vec{d}, \vec{G}}$
    \item Randomly sample $x, q$
    \item Run the norm prover on $P(x, q) + D + 2 \sum_{i=0}^n x^{i+1} C_i$ with $s_s = 1$ and $q$
\end{enumerate}

\section{Reciprocal Argument}

Naively trying to extend the binary digit check to multiple bases, we might try to simply use a larger polynomial. In the binary case $x (x - 1)$ is zero on valid binary digits, so perhaps using $x (x - 1) (x - 2) ...$ would work for larger bases. Unfortunately, this scales the number of multiplications per digit of the range linearly in the size of the base and is actually less efficient than the binary case for larger bases since the number of digits decreases only logarithmically in the base.

Rather than trying to show that each digit is a zero of some polynomial, we can work around this problem by first constructing a polynomial whose zeros are the digits with multiplicity. That is, for base $b$ digits $\vec{d}$ we construct the polynomial $f(X)$ as

$$ f(X) = \prod_{i=0}^n (X - d_i) = \prod_{j=0}^{b-1} (X - j)^{m_j} $$

Where $m_j$ is the multiplicity of the symbol $j$ among the digits. As long as the multiplicities are small compared to the field characteristic, we can take the logarithmic derivative of this equation and find

$$ \frac{f'(X)}{f(X)} = \sum_{i=0}^n \frac{1}{X - d_i} = \sum_{j=0}^{b-1} \frac{m_j}{X - j} $$

This suggests a more efficient zero knowledge proof protocol: commit to the digits $d_i$ and the symbol multiplicities $m_j$, choose a challenge value for $X$, which we will canonically call $e$ in the proofs, and then commit to values 

$$ r_i = \frac{1}{e - d_i} $$

And show that

$$ (e - d_i) r_i = 1 \quad \text{ and } \quad \sum_{i=0}^n r_i = \sum_{j=0}^{b-1} \frac{m_j}{e - j} $$

The soundness and completeness of this argument follow straightforwardly from the Schwartz-Zippel lemma. It is sufficient to show that in the more general case of an arbitrary rational function, given $e$ is drawn from a set of $N$ values

$$ P\left(u(X) \neq 0 \Bigm| \frac{u(e)}{v(e)} = 0\right) \leq \frac{\deg(u v)}{N} $$

It is necessary to include the denominator in general, especially when the reciprocal checks do not always have numerator equal to one. For example, suppose the prover committed to both some roots $\vec{t}$ and some numerators $\vec{v}$ and checked that $(e - t_i) r = v_i$. If the prover set one of the numerators to zero and the challenge were equal to $t_i$ this would allow the prover to violate soundness by arbitrarily varying $r$ since $0 r = 0$ for any value. 

\section{Reciprocal Range Proofs}

There are two ways to use the reciprocal argument to construct range proofs that are efficiently compatible with multiparty proving. Either each range with use its own set of symbols and multiplicities on the same basis elements as its digits, or multiple ranges can share basis elements in the linear portion of the norm linear argument. The reason multiple ranges cannot share basis elements and overlap with digits is because of the complexity of secure multiparty computation of field multiplications. 

To see why ranges might want to share digits, consider the a case where a prover wants to show that a large number of values all belong to a small range, for example $N$ values such that $0 \leq v_i < 2^8$. Individually, the optimal base for each range is $4$ since $4^4 = 2^8$. If we allow the prover(s) to share digits, this base can be increase all the way up to $256 = 2^8$. This allows the prover to reduce the witness length to a total of $256 + N$ values from $4 N$. For a  large number of values, $N > 256/3$, this can result in a substantial reduction in witness length. 

\subsection{Arbitrary Ranges}

While the arbitrary range condition is a nice to have feature for binary range proofs, it is absolutely necessary for arbitrary bases. In practice, many ranges are fixed externally to the proof system and often do not have nice integer solutions like $4^4 = 2^8$ or $16^{16} = 2^{64}$ that allow expressing the range as the power of a larger base, e.g. $10^9 < 2^{32} < 10^{10}$. Without loss of generality with respect to the lower bound, for a range$0 \leq v < B$ in base $b$ there are three cases.

\subsubsection{$b - 1 \mid B - 1$}

This is the natural generalization of the binary case, in the sense that $2 - 1 = 1$ divides every range. For arbitrary bases this is obviously not the case, but when it is we can use the base vector $\vec{b}$ 

$$ b_i = b^i \quad \text{for} \quad i < n-1 \qquad b_{n-1} = \frac{B - b^{n-1}}{b - 1} $$

Expanding out the maximum representable value, we find

$$\sum_{i=0}^{n-1} (b - 1) b_i = (b^{n-1} - 1) + (B - b^{n-1}) = B - 1$$

\subsubsection{$b^{n-1} < B \leq 2 b^{n-1}$}

When the range is not divisible by $b-1$ but is sufficiently close to the lower power of $b$ we can still use $n$ digits by making the last digit binary. To see why this works, observe that the subrange spanned by the first $n-1$ digits is $[0, b^{n-1})$. Since this is at least half the distance to $B$, we can overlay the same range shifted up by $B - b^{n-1}$ and cover the whole range $[0, B)$. Thus in this case we let 

$$ b_i = b^i \quad \text{for} \quad i < n-1 \qquad b_{n-1} = B - b^{n-1} $$

\subsubsection{$2 b^{n-1} < B < b^n$}

In the final case, we need to add an additional digit increasing the length of $\vec{b}$ to $n+1$ but cannot let $b_{n-1} = b^{n-1}$, as this would allow representing values outside the range. Instead, we can choose some value so that the maximum representable value with the first $n$ digits is at least half way to $B$ and then use another binary digit like the previous case.

\begin{gather*}
    b_i = b^i \quad \text{for} \quad i < n-1 \\
    b_{n-1} = \ceil{\frac{B-1}{2(b - 1)}} - \frac{b^{n-1} - 1}{b - 1} \qquad b_n = B - (b - 1) b_{n-1} - b^{n-1}
\end{gather*}



\subsection{Inline Multiplicities}

We will consider the more complex case first where the multiplicities are inline with the digits. Suppose $\vec{b}$ is defined as above for the base $b$ and the range $A \leq v < B$. Then let the vector $\vec{d}$ be a vector of base $b$ digits, potentially with a final binary digit if it is necessary for the range, such that $\ip{\vec{b}}{\vec{d}} = v$. Let the vector $\vec{m}$ be defined in such that $m_i$ for $i=0..b-2$ is the number of occurrences of the symbol $i+1$ among the base $b$ digits in the value $v$. If there is a binary digit, let $m_{b-1}$ be equal to the binary digit. 

Notice that we do not store a multiplicity for the zero symbol for the binary digits or the base $b$ digits. This is because the total number of digits is a public constant, so the number of zero digits will always be the total number of digits minus the number of nonzero digits. Consider for simplicity the case where there is no bit first. We can test

$$\sum_{i=0}^{\len{\vec{d}}-1} \frac{1}{X + d_i} - \sum_{j=0}^{\len{\vec{m}}-1} m_j \left(\frac{1}{X + (j+1)} - \frac{1}{X} \right) = \frac{\len{\vec{d}}}{X}$$

When there is a bit, we can perform the same check on the binary digit separately. The core of the reciprocal argument remains the same as before: the prover will commit to the digits and the multiplicities, the verifier will choose the challenge $e$, and the prover will then commit to the reciprocals. To actually show this, like earlier bulletproof protocols, we can organize the commitments into a polynomial and show that some the coefficients of this polynomial encode the desired constraints. In particular, let $\vec{u}(x) = x^2 \vec{b}$ and the vectors $\vec{v}(x)$, and $\vec{c}(x)$ be defined so that 

$$ v_i(x) = \begin{cases}
    x^3 & \mbox{ if } d_i \text{ is base } 2 \\
    x^5 & \mbox{ if } d_i \text{ id base } b \\
    0   & \mbox{ if fewer than } i+1 \text{ digits}
\end{cases} \qquad
c_i(x) = \begin{cases}
    \frac{x^3}{e} - \frac{x^3}{e + (i+1)} & \mbox{ if } 0 \leq i < b-1 \\
    \frac{x^5}{e} - \frac{x^5}{e + 1}       & \mbox{ if } i = b-1 \text{ and range has bit } \\
    0 					  & \mbox{ if } \vec{m} \text{ undefined }
\end{cases} $$

Also let the vector $\vec{a}$ be $1$ on entries where digits are defined, i.e. whenever $\vec{b}$ is defined, and $0$ elsewhere. This allows us to define the vector valued polynomial

$$ \vec{p}(y) = y Q \vec{m} + y^3 (Q \vec{r} + Q^{-1} \vec{u}(x)) + y^5 (Q (e \vec{1} + \vec{d}) + Q^{-1} \vec{v}(x)) + y^7 Q^{-1} \vec{c} $$

Where the $y^8$ term of the norm of $\vec{p}(y)$ is

$$ \left[y^8\right] \norm{\vec{p}(y)} = 2 \left( x^2 \ip{\vec{d}}{\vec{b}} + \left( \ip{\vec{r}}{\vec{v}(x)} + \ip{\vec{m}}{\vec{c}(x)} \right) + \ip{Q \vec{r}}{Q (e \vec{1} + \vec{d})} \right) $$

Breaking this polynomial down further in terms of $x$ powers, we see the $x^2$ term encodes the value, the $x^3$ term encodes the base $2$ reciprocal sum, and the $x^5$ term encodes the base $b$ reciprocal sum. The constant $x$ term encodes multiplicative constraints as a polynomial in $q$. So, if the following holds, we can deduce that all the digits are of the correct base and sum with the bases to product the correct value

$$ \left[y^8\right] \norm{\vec{p}(y)} = 2 v x^2 + \frac{2 \ip{\vec{v}(x)}{\vec{1}}}{e} + 2 \norm{Q \vec{a}} $$

We can easily extend this to check to multiple values with different bases by concatenating the vectors of each value, scaling the base values of each value by distinct even powers of $x$, and scaling distinct base digits by distinct odd powers of $x$. Also note that the $\vec{d}$, $\vec{m}$, and $\vec{r}$ must be padded to be the same length.

\subsubsection{Error Terms}

While the $y^8$ term encodes the constraints for the proof, the norm of $\vec{p}(y)$ has $6$ other terms, although $y^{14}$ is computable from public information. The rest of the terms are ``error terms" that don't encode any useful information directly but must be accounted for to prove that the $y^8$ term is what it must be for the proof to hold.

Among the error terms $y^{12}$ is unique since it does not involve any multiplications of committed values with non trivial powers of $q$. If we choose $x$ and $e$ at the same time, then the prover will be able to compute and include this error term in the scalar of the $\vec{r}$ commitment. Scaling the scalar by $y^9$, i.e. $s_s = y^{-9}$ will place the $\vec{r}$ scalar on the $y^{12}$ term since the $\vec{r}$ commitment is already scaled by $y^3$. We can additionally see that the $\vec{d}$ and $\vec{m}$ scalars are not scaled to $y^8$, so this will not affect the constraint term.

The remaining error terms will be included in the linear vector, but since they depend on $q$ they cannot be included in the $\vec{r}$ commitment. Instead, they can be incorporated into the first blinding commitment. After choosing $q$ but before choosing $y$, the prover will compute the error terms and commit to them with the blinding vector for the norm vector. After the prover chooses $y$, the prover will be able to commit to blinding for these linear coefficients in the second blinding commitment so that the when the linear terms are scaled by the inverse of the blinding challenge the verification equation holds. See the protocol for details. 

\subsubsection{Protocol}

First we need to define the public terms that will be computed by the verifier and depend only on challenges and public information. Some of the information in the public terms overlaps with the error terms, although it doesn't matter for correctness whether it is evaluated publicly or included in an error term commitment. It may be more performant for the verifier to include the overlapping values in the error term commitments.

$$ \vec{n}_{pub}(x, q, y) = y^3 Q^{-2} \vec{u}(x) + y^5 (e \vec{1} + Q^{-2} \vec{v}(x)) + y^7 \vec{c}(x) $$
$$ s_{pub}(x, q, y) = \norm{\vec{n}_{pub}(x,y)} + \norm{Q \vec{a}} + 2 y^8 \sum_{j=0}^k \frac{\ip{\vec{v}(x)}{\vec{1}}}{e} - A_j x^{2j + 2} $$
$$ P(x, q, y) = y^{-9} s_{pub}(x,q,y) G + \ip{\vec{n}_{pub}(x, q, y)}{\vec{G}} $$

With this definition of the public terms, the error terms $\epsilon_2$, $\epsilon_4$, $\epsilon_6$, $\epsilon_{10}$, and $\epsilon_{12}$ are

\begin{align}
    \epsilon_2    &= \norm{Q \vec{m}} \\
    \epsilon_4    &= 2 \ip{Q \vec{m}}{Q \vec{r}} + 2 \ip{\vec{m}}{\vec{u}(x)} \\
    \epsilon_6    &= \norm{Q \vec{r}} + 2 \ip{\vec{r}}{\vec{u}(x)} \\
    \epsilon_{10} &= \norm{Q \vec{d}} + 2 \ip{e Q \vec{1} + Q^{-1} \vec{v}(x)}{Q \vec{d}} + 2 \ip{\vec{r}}{\vec{c}(x)} \\
    \epsilon_{12} &= \ip{\vec{d}}{\vec{c}(x)}
\end{align}

The inline digit range proof protocol is then as follows 

\begin{enumerate}
    \item Input commitments $C_j = \com{}{v_j}$, bases $b_j$, and ranges $[A_j, B_j)$ for $j = 0..k$
    \item Let $\vec{b} = \bigoplus_{j=0}^k \vec{b}_j$, $\vec{m} = \bigoplus_{j=0}^k \vec{m}_j$, $\vec{d} = \bigoplus_{j=0}^k \vec{d}_j$ for $(\vec{b}_j, \vec{m}_j, \vec{d}_j)$ defined as above and padded with zeros to minimal equal length
    \item Commit $D = \com{}{\vec{d}, \vec{G}}$ and $M = \com{}{\vec{m}, \vec{G}}$
    \item Choose $e, x \leftarrow \mathbb{F}_p$
    \item Let $r_i = (e + d_i)^{-1}$ if digit is defined (i.e. $b_i \neq 0$), zero otherwise
    \item Commit $R = \com{}{\epsilon_{12}; \vec{r}, \vec{G}}$
    \item Choose $q \leftarrow \mathbb{F}_p$
    \item Prover chooses random blinding $s$ and $\vec{s}_n$
    \item Commit $R_1 = \com{}{s; (\epsilon_2, \epsilon_4, \epsilon_6, \epsilon_{10}), \vec{H}; \vec{s}_n, \vec{G}}$
    \item Choose $y \leftarrow \mathbb{F}_p$
    \item Let $W = P(x, q, y) + y M + y^3 R + y^5 D + 2 y^{-1} \sum_{j=0}^k x^{2 j + 2} C_j = \com{}{w; \vec{w}, \vec{G}}$
    \item Prover chooses random blinding $s_4, s_6, s_{10}$
    \item Let $s_2 = 2 y^{-2} \ip{Q \vec{w}}{Q \vec{s}_n} - s y^7 - \left( s_4 y^2 + s_6 y^4 + s_{10} y^8 \right)$ where 
    \item Commit $R_2 = \com{}{\norm{Q \vec{s}_n}; (s_2, s_4, s_6, s_{10}), \vec{H}}$
    \item Choose $t \leftarrow \mathbb{F}_p$
    \item Let $\vec{l}_c = -t^{-1} (y^2, y^4, y^6, y^{10})$
    \item Let $C = W + t R_1 + t^2 R_2$
    \item Norm Linear argument on $C$ with $s_s = y^{-9}$, $\vec{l}_c$ and $q$
\end{enumerate}


\subsection{Shared Digits}

The inline digit protocol can be adapted to share digits with very little modification and is fully compatible in the sense that the prover can construct a single range proof where some values use shared digits and others use inline digits. For the shared digits, the prover will commit to the combined multiplicities of each symbol for each base in the linear vector and use the corresponding $\vec{c}(x)$ values as the linear coefficients for these values. Note that as defined $\vec{c}(x)$ for multiplicities for the same symbol from the same base is the same. Let multiplicity and coefficient vectors be $\vec{m}^{(s)}$ and $\vec{c}^{(s)}(x)$ for clarity. The components of the $\vec{c}(x)$ coefficients, for the shared digits in the norm vector will be zeroed. If all the bases are shared, the entire $\vec{c}(x)$ will be zero and the $M$ commitment and $\epsilon_2$, $\epsilon_4$, and $\epsilon_{12}$ error terms can all be dropped as they are zero by construction if there are no inline digits.

\subsubsection{Protocol}

The primary difference from the inline digit protocol is that some of the multiplicity vectors lie over the linear bases. This does not actually require changing much about the inline digit protocol except for extending the linear coefficient vector and committing to these multiplicities in the $D$ commitment, as well some straightforward modification of the blinding. These are only the steps from the inline protocol that differ in the shared base case.

\begin{enumerate}
    \addtocounter{enumi}{1}
    \item Define $\vec{b}, \vec{d}, \vec{m}$ as before but let $\vec{m}^{(s)}$ be the shared base multiplicities and let $\vec{m}$ be zero for the values whose bases are shared. Similarly, let $\vec{c}^{(s)}(x)$ be the $\vec{c}(x)$ for the shared bases and let $\vec{c}(x)$ be zero for shared base values.
    \item Commit $D = \com{}{\vec{m}^{(s)}, \vec{H}; \vec{d}, \vec{G}}$ and $M = \com{}{\vec{m}, \vec{G}}$
    \addtocounter{enumi}{4}
    \item Prover chooses random blinding $s_s$, $\vec{s}_l$, and $\vec{s}_n$
    \item Let $R_1 = \com{}{s_s; (\epsilon_2, \epsilon_4, \epsilon_6, \epsilon_{10}) \oplus \vec{s}_l, \vec{H}; \vec{s}_n, \vec{G}}$
    \addtocounter{enumi}{1}
    \item Let $W = P(x, q, y) + y M + y^3 R + y^5 D + 2 y^{-1} \sum_{j=0}^k x^{2 j + 2} C_j = \com{}{w; \vec{w}, \vec{G}}$
    \addtocounter{enumi}{1}
    \item Let $s_2 = 2 y^{-2} \ip{Q \vec{w}}{Q \vec{s}_n} + 2 y \ip{\vec{c}^{(s)}(x)}{\vec{s}_l} - s_s y^7 - \left( s_4 y^2 + s_6 y^4 + s_{10} y^8 \right)$
    \item Commit $R_2 = \com{}{\norm{Q \vec{s}_n}; (s_2, s_4, s_6, s_{10}) \oplus \vec{0}, \vec{H}}$
    \addtocounter{enumi}{1}
    \item Let $\vec{l}_c = -t^{-1} (y^2, y^4, y^6, y^{10}) \oplus \vec{c}^{(s)}(x)$
\end{enumerate}

\section{Confidential Transactions} 

One of the most common real world use cases for range proofs is implementing confidential transaction protocols in cryptocurrencies. In such a protocol, the amount of cryptocurrency involved in each transaction is kept secret, and the prover instead publishes a zero knowledge proof that the transaction is valid. Range proofs are one of the fundamental building blocks of confidential transactions as they prevent fraudulently creating money by creating a transaction output or account balance that is negative. It is important to note that confidential transactions do not hide the transaction meta data, that is relationships between transactions, only some of the information within each transaction. Besides the obvious privacy benefits of confidential transactions, they can also help defend against targeted phishing attempts and cyber attacks by making it harder for attackers to easily identify high value targets.

Some transaction protocols involve more information than simple amounts of a single kind of cryptocurrency. For example, many blockchains allow multiple types of tokens that may have different issuance rules and are not fungible. The same reciprocal argument used in the range proofs can be used to construct a more complex confidential transaction protocol for these sorts of multi-type transactions. Such a protocol should not reveal any information about the types of token involved in the transaction apart from what can be inferred from the number of inputs and outputs, which obviously bounds the number of possible types, and transaction metadata. This hides not only which types are involved in the transaction, but also which inputs and outputs of the transaction are of the same or different types.

\subsection{Argument}

Given two sets $I$ and $O$ of amounts and types, the inputs and outputs respectively, the prover wants to show that the amounts of all the inputs of each type add up to the same value as the amounts of all the outputs of the same type for all types. We can encode this as a reciprocal relation by using the type of each commitment as the symbol and each amount as the multiplicity for a reciprocal term like so

$$ \sum_{(v,t) \in I} \frac{v}{e + t} - \sum_{(v,t) \in O} \frac{v}{e + t} = 0 $$

While the reciprocal argument in the range proof is more efficient than the polynomial argument, the preimage of the logarithmic derivative, the multiplicities tend to be small. If the prover really wanted to, it would be feasible to use the polynomial argument instead. Here, the power of the reciprocal argument is much more apparent, as trying to use an equivalent polynomial argument would likely be impractically expensive.

Translating this into a system of constraints, we can follow the same recipe as the range proofs, more or less. The prover will commit to $\vec{t}$ and $\vec{v}$, the verifier will choose $e$, and the prover will commit to a vector of reciprocals such that

$$ r_i = \frac{v_i}{e + t_i} $$

Because of the similarity in structure to the range proofs, we can add actually prepend the $\vec{t}$ vector to the vector of digits and the type reciprocals to the vector of digit reciprocals. These reciprocals are importantly different since their numerator is not one. To compensate for the numerators, the prover can scale each input by $x^{2 (i+1)} + q^{2 (i+1)}$ and let $\vec{a}$ be zero on the type reciprocals.

These coefficients will also scale the types in the value commitments, and the resulting sum of types will occur on the first linear vector basis element. Scaling this sum by $-q$ will place each type on a distinct odd power of $q$ and the $x$ power sum of types on $q$. Finally, we can modify the $\vec{u}(x)$ to multiply the types in the norm vector by $q (x^{2 (i+1)} + q^{2 (i+1)})$ to verify that these value are equal to the types in the commitments. We must also modify $\vec{v}(x)$ so that input reciprocals and output reciprocals are scaled by opposite sign.

$$ u_i(x, q) = q x^{2 i+ 2} + q^{2 i+3} \text{ for } i = 0..k-1 $$
$$ v_i(x) = \begin{cases}
  x  & \text{ if commitment } i \text{ is input} \\
  -x & \text{ if commitment } i \text{ is output}
\end{cases} \qquad c_i(x) = 0 \text{ for } i = 0..k-1 $$

\subsection{Protocol}

Once again, I will only describe the ways in which the typed confidential transaction protocol differs from the inline/shared digit range proof. The public constants are actually the same once again using appropriately redefined coefficient vectors and taking case to note that even though the types are multiplied, there is no corresponding $q$ power that comes from the multiplication. 

\begin{enumerate}
    \item Input commitments $C_j = \com{}{v_j; t_j \oplus \vec{0}, \vec{H}}$, bases $b_j$, and ranges $[A_j, B_j)$ for $j = 0..k$
    \item Define $\vec{d}$, $\vec{m}$, and $\vec{b}$ as in the range proof, but prepend the vector of types $\vec{t}$ to $\vec{d}$ and zeros to $\vec{m}$ and $\vec{b}$
    \addtocounter{enumi}{5}
    \item Prover chooses random blinding $s$, $s_t$, $\vec{s}_l$, and $\vec{s}_n$
    \item Let $R_1 = \com{}{s; s_t \oplus (\epsilon_2, \epsilon_4, \epsilon_6, \epsilon_{10}) \oplus \vec{s}_l, \vec{H}; \vec{s}_n, \vec{G}}$
    \addtocounter{enumi}{1}
    \item Let $W = P(x, q, y) + y M + y^3 R + y^5 D + 2 y^{-1} \sum_{j=0}^k (x^{2 j + 2} + q^{2j + 2}) C_j = \com{}{w; \vec{w}_l, \vec{H}; \vec{w}, \vec{G}}$
    \addtocounter{enumi}{1}
    \item Let $s_2 = 2 y^{-2} \ip{Q \vec{w}}{Q \vec{s}_n} + 2 y \ip{\vec{c}^{(s)}(x)}{\vec{s}_l} - (s + q s_t) y^7 - \left( s_4 y^2 + s_6 y^4 + s_{10} y^8 \right)$
	\item Commit $R_2 = \com{}{\norm{Q \vec{s}_n}; 0 \oplus (s_2, s_4, s_6, s_{10}) \oplus \vec{0}, \vec{H}}$
    \addtocounter{enumi}{1}
    \item Let $\vec{l}_c = -q y^9 \oplus -t^{-1} (y^2, y^4, y^6, y^{10}) \oplus \vec{c}^{(s)}(x)$
\end{enumerate}

In practice, it is typically not necessary to prove that the transaction inputs are in a given range since they are usually the outputs of an earlier transaction which will have already shown their validity. In that case, the prover can remove the digit vectors for the inputs but retain the types. This requires modifying the coefficients of the input commitments to not scale by $x^{2 i}$ and similarly to modify the $\vec{u}(x)$ of the types of the inputs. Otherwise the proof is the same. Similarly, public transaction inputs or outputs can just be directly added to or subtracted from the public scalar.

\section{Arithmetic Circuits}

To show that, given public constraints matrices $A, B, C \in \mathbb{F}_p^{n \times m}$ public $\vec{d} \in \mathbb{F}_p^m$, and matrix $D \in \mathbb{F}^{k \times m}$ with empty right null space, i.e. $D \vec{x} = \vec{0}$ only if $\vec{x} = \vec{0}$, that the following relation holds

$$ A \vec{a} + B \vec{b} + C \vec{c} = D \vec{v} + \vec{d} \qquad a_i b_i = c_i$$

For input commitments $C_i = \com{}{v_i}$, we can use a protocol that is actually quite similar to the structure of the reciprocal range proof. After committing to the witness, sample $r$ at random with $r_i = r^{i+1}$ in $\vec{r} \in \mathbb{F}_p^m$ to linearly combine the linear constraints. Like the reciprocal range proofs, we will then construct the polynomial

$$ \vec{p}(y) = y Q \vec{c} + y^3 (Q \vec{a} + Q^{-1} B^{\top} \vec{r}) + y^5 (Q \vec{b} + Q^{-1} A^{\top} \vec{r}) + y^7 (Q^{-1} C^{\top} \vec{r} - Q \vec{1}) $$

This places the constraints on the $y^8$ coefficient once again, puts the multiplicative constraints on the positive, even $q$ powers within the term, and puts the linear constraint on the constant $q$ term. The inputs are linearly combined with $D^{\top} \vec{r}$ and the public constants are also added. This polynomial has five error terms and all of them must be included in the linear vector of the first blinding commitment. 

$$ \left[ y^8  \right] \vec{p}(y) = \vec{r}^{\top} (A \vec{a} + B \vec{b} + C \vec{c}) + \ip{Q \vec{a}}{Q \vec{b}} - \ip{Q \vec{1}}{Q \vec{c}} $$

Given $n = \ceil{\log_2 \len{\vec{a}}}$ and $n > 3$, this will use $n - 2$ rounds, have $5$ scalars, $3$ commitments to the witness, and two blinding commitments. This gives a total proof size of $6 s + (1 + 2 n) g$. In particular, this proof size is $32$ bytes smaller than the comparable Bulletproof+ and the witness length is one third. This is a relatively small improvement overall in proof size, although the shorter witness length will make Bulletproof++ range proofs faster for the verifier and likely the prover as well.

The $\vec{c}$ constraints can also be placed in the linear portion of the proof. In this case, we can eliminate all but two of the error terms and split the $\vec{c}$ in half between the $\vec{a}$ and $\vec{b}$ commitments eliminating the $\vec{c}$ commitment and saving a group element. If the length of these vectors $n$ satisfies $2^{k-1} < n \leq 2^{k-2} - 4$ then after $k-1$ rounds of the norm linear argument there will be one linear scalar and two norm scalars, which results in an overall proof size one group element smaller. This form does increase the witness length by fifty percent compared to the separate commitment form.

$$C_a = \com{}{\vec{c}_0, \vec{H}; \vec{a}, \vec{G}} \qquad C_b = \com{}{\vec{c}_1, \vec{H}; \vec{b}, \vec{G}} $$
$$\vec{l}_c = y^3 \left( Q^{-1} C^{\top} \vec{r} - Q \vec{1} \right)_1 + y^5 \left( Q^{-1} C^{\top} \vec{r} - Q \vec{1} \right)_0 $$

\section{Performance}

\textbf{Note: incomplete}

The statistics for Bulletproofs++ binary range proofs are relatively straightforward to calculate explicitly. The proof size is always $2$ group elements smaller than an equivalent inner product proof and proving time should be roughly the same. This is because computing the commitment $X$ requires asymptotically twice as many elliptic curve operations as $L$, but the commitment length is also reduced by half. Verification time should be approximately fifty percent faster as measured in elliptic curve operations.

The reciprocal range proofs are more complicated since the witnesses length and associated proof size will depend on the base used. Asymptotically this turns out to not really make a difference, although the constant factors may matter in practice. A $64$ bit, base $16$ inline digit reciprocal range proof for $k$ values will be of size 

$$(9 + 2\ceil{\log_2 k}) g + 6 s$$

Which, using Curve25519, is $480$ bytes for one value which is $64$ bytes smaller than a $64$ bit Bulletproof+ at $576$ and $160$ bytes smaller than a $64$ bit Bulletproof. As the number of values increases, this constant difference in size persists with inline digit proofs.

In the case of larger number of values, it becomes efficient to use a shared digit proof. In the case, for example, of $128$ $64$ bit values, we can use the base $256 = 2^8$ and use $8$ digits per value since $2^{64} = 256^8$. In that case, we will have a norm vector of length $1024$ and a linear vector of length $2 + 256$. After $9$ rounds of the norm linear argument we have $22 g + 4 s$ or $832$ bytes for Curve25519. This is $288$ bytes smaller than the Bulletproof range proof and $174$ bytes smaller than the Bulletproof+ range proof for the same number of $64$ bit ranges.

Perhaps more importantly, the witness length is over $12$ times smaller than that of the inner product arguments. That is, for $1024 + 258$ = $1282$ versus $16384$ for the inner product arguments. Since both proving and verification computation are overwhelmingly dominated by elliptic curve operations, this reduction in witness length makes proving and, often more importantly in practice, verification many times faster. 

The optimization I have introduced for the norm linear argument, i.e. reducing the basis update complexity and avoiding the explicit power scaling of basis elements, are also applicable to the inner product protocols, are also applicable to the norm linear argument. In light of this, it may be more fair to compare the performance of the reciprocal range proofs to a modified inner product argument to use these optimizations. Additionally, I do not make use of the complex multiplication based optimizations for the basis update as these are only available on some curves.

\section{Proofs}

\textbf{Note: incomplete}

As in Bulletproofs(+), there are three, or four if we count soundness and completeness separately, properties that the Bulletproof++ protocols must satisfy. These are soundness and completeness, special honest verifier zero knowledge, and computational witness extractability. Completeness is the property that every valid witness can be proven, soundness the property that every invalid witness cannot be proven both with overwhelming probability. SHVZK and CWE are essentially a formal way to show that given a proof the verifier learns nothing about the witness and that a prover must know a valid witness to construct a proof.

In the random oracle model, this is shown by manipulating the oracle and either allowing a prover simulator to know the challenges in advance to produce a simulated proof with the challenges that does not depend on the witness, in the SHVZK case, or allowing the verifier to query the oracle for multiple different challenges at each point in the protocol to solve for the witness, in the CWE case. These proofs tend to be somewhat formulaic, so I will try to given them in a general form that is sufficient to show that all the Bulletproofs++ protocols are SHVZK and CWE. Soundness and completeness are less generalizable.

\subsection{General Lemmas for SHVZK and CWE}

If every commitment is blinded uniformly at random, and every open scalar is blinded uniformly at random, then the protocol is SHVZK. 

If round involves computing a linear combination of the commitments such that every scalar coefficient is a rational function in the challenges and public constants and the linear combination is of full rank over the field of rational functions in all the prior challenges, then the protocol is CWE. This follows basically fro

\subsection{Norm Linear Argument}

\subsubsection{SHVZK}

While all the commitments in the norm linear argument are blinded, the protocol is not by itself zero knowledge unless the vector is blinded at some point. Assuming that the vector is blinded, however, it is easy to produce identically distributed transcript.

\subsubsection{Witness Extraction}

In basically exactly the same way as Bulletproofs, we can show CWE by showing that the witness for each round can be extracted from an opening of its output. To do this, we rewind each round three times to extract three values of $e$ for a given round and then solve the resulting linear system at each component at the vector. Since the linear coefficients $1$, $e$, and $e^2 - 1$ are linearly independent with respect to the base field and all the previous challenges, this is possible with overwhelming probability and will yield the witness of the input commitment. 

\subsection{Reciprocal Range Proofs and Confidential Transactions}

Since the Confidential transaction protocol is a superset of the shared digit range proof, which is a superset of the inline digit range proof, it is sufficient to show that the confidential transaction protocol satisfies the desired properties in the random oracle model to show that the other reciprocal range proofs also satisfy the properties. 

\subsubsection{Perfect Completeness}

Perfect completeness follows trivially from the fact that all the equations hold algebraically and the hardness of the discrete log problem.

\subsubsection{SHVZK}

This follows from the norm linear argument, that all commitments are blinded, and all scalar openings are also blinded. 

\subsubsection{Witness Extraction}

From the CWE of the norm linear argument, we must show that from the final expression $W + t R_1 + t^2 R_2$ that we can extract openings of the initial commitments. We can extract openings for $W$, $R_1$, and $R_2$ by evaluating at $3$ values of $t$. Rewinding further, we are able to extract the witness of every term in the $y$ polynomial as all the $y$ powers are linearly independent over $\mathbb{F}_p(e, x, q)$. We are now left with an opening of the expression 

$$ \sum_{j=0}^k (x^{2 j + 2} + q^{2j + 2}) C_j $$

We can extract openings for each of these commitments by further rewinding to the selection of $x$ and running $k+1$ times. This yields an opening of every commitment in the protocol including the initial commitments and therefore the confidential transaction protocol is CWE.

% Old
\newpage
\newpage

\iffalse
\section{Reciprocal Argument}

Many general zero knowledge proof protocols work by first constructing a zero knowledge proof system for arithmetic circuits and then translating the statement to prove into an arithmetic circuit. This technique has the advantage of being generic and easy applicable to new problems but sometimes fails to exploit the full potential of zero knowledge proof systems. These failures occur in two ways that are of particular interest here: by neglecting opportunities for more efficient probabilistic arguments in favor of mechanistic translations of deterministic logic and by artificially restricting the proofs, both via the proof systems and tooling, to deterministic, imperative computations when a declarative style would be more natural and efficiently transformable into a circuit.

As a motivating example that we will use in the construction of arbitrary base range proofs later, consider a hypothetical ``choose" operation, which denotes a place in the circuit where the prover can select any of a number of values for a computation. This operation doesn't really make sense in an imperative setting, although it can be expressed naturally in a declarative language like Prolog. To describe such an operation using a circuit, we could construct a polynomial with roots at all the candidate values and show that the polynomial vanishes at the choice. However, the multiplicative complexity of this circuit scales linearly in the number of choices and the number of choose operations.

We can do much better using a probabilistic argument. Consider the case where a prover wants to show that a multiset of values $v_1, .., v_n$ all lie in some set $x_1, .., x_k$. Rather than constructing a polynomial with roots at the $x_i$, we can construct a polynomial with roots at the $v_i$. Taking the logarithmic derivative of this polynomial $f(v_i) = 0$ we find

$$ \frac{d}{dt} \log f(t) = \frac{f'(t)}{f(t)} = \sum_{i=1}^n \frac{1}{t - v_i} = \sum_{j=1}^k \frac{m_j}{t - x_j} $$

Here the $m_j$ values are the multiplicities of each root and count the number of occurrences of $x_j$ in the multiset of $v_i$ values. Using the Schwartz-Zippel lemma, we can show with overwhelming probability that this equality of rational functions holds by showing that it holds at a single randomly chosen $t$ value. Applying the Schwartz-Zippel lemma to a rational function, in general, requires that we consider the combined degree of the numerator and denominator, depending on the particular algorithm.

In this case, we can sketch out a protocol, and evaluate the soundness, as follows. Suppose the prover commits to $\vec{v}$, $\vec{m}$, and $\vec{x}$ and the verifier chooses a random $t$. The prover will respond by committing to a pair of vectors $\vec{a}$ and $\vec{b}$ and, using a proof system for arithmetic circuits, show

$$ a_i (t - v_i) = 1 \qquad b_j (t - x_j) = m_j $$
$$ \sum_{i=1}^n a_i = \sum_{j=1}^k b_j $$

In this case, the soundness error for the Schwartz-Zippel lemma comes from the probability that $m_j = 0$ and $t = x_j$ for some $j$. This would allow the prover to submit an arbitrary $b_j$ value and construct an invalid proof. Since the right hand side of the $a_i$ equations is fixed, an analogous error is not possible, since if $t = v_i$ the constraint would simply not be satisfiable. Thus, for a $T$ drawn uniformly at random from $N$ values, the soundness error is $N^{-1} (n + 2*k - 1)$, where the factor comes from the degree of the combined numerator plus the $b_j$ multiplicative constraints. In any event, as long as both the number of choices and number of operations are negligible, the argument is sound with negligible probability. 

\subsection{Range Proofs in Larger Bases}

One natural application of this technique, and the subject of the next section of this paper, is to construct a range proof over a larger base than binary. Binary digits conveniently admit a quadratic check, since $b * (1-b) = 0$ for $b = 0, 1$. However the degree of this check increases linearly in the size of the base. Instead of checking each digit with a polynomial, the prover can show that each digit belongs to the set of valid digits using a reciprocal proof.

For example, a value from the range $0 \leq v < 2^64$ can be expressed using $64$ binary digits, or using $16$ hexadecimal digits. We can then let $d_i$ be the hex value of the $16^i$ digit, let $x_j = j$ be the digit value $j$, and let the $m_j$ be the number of occurrences of the $j$ digit. Checking the validity of all the digits uses only $32$ multiplications to show

$$ \sum_{i=0}^{16} \frac{1}{t - d_i} = \sum_{j=0}^{16} \frac{m_j}{t - j} $$

This can be generalized to multiple values by either letting each value have its own set of digits or by allowing values to share the set of digits. In the latter case, as the number of values increases, we can increase the size of the base while decreasing the overall size of the proof. Consider, for example, a range proof that $n$ values lie in $0 \leq v_i < 2^8$. If each digit has their own set, the optimal number of digits per value is $4$ since $2^8 = 4^4$. Letting the values share the set of digits, we can increase the base all the way to $256$ using one digit per value. As $n$ grows the total number of multiplications approaches $n + 256$, which is $8$ times smaller than the proof where each value has its own set.

\subsection{Conservation of Money}

Confidential transactions [] are a class of cryptocurrency transaction protocols where the amounts of money in each transaction input and output are kept secret. To do this, the transaction publishes a range proof for every output of the transaction, inheriting the range proofs for the inputs from their minting transactions, and proves in zero knowledge that the total amount of money flowing into the transaction is equal to the amount flowing out, i.e. conservation of money. 

For a transaction dealing with a single type of currency, proofs of conservation of money can be extremely simple due to the linearity of Pedersen commitments, as in MimbleWimble []. However, for multiple types of currency in the same transaction, the situation is more complex. The prover can't just sum Pedersen commitments , since this would reveal which inputs and outputs are of the same type even if the type itself were kept secret and would also reveal the number of types involved in the transaction. To keep this information private, the prover(s) of the transaction can use a modified reciprocal proof. Letting $I$ and $O$ be the sets of amounts and types $(v, t)$ respectively, the prover can show

$$ \sum_{(v, t) \in I} \frac{v}{X - t} = \sum_{(v, t) \in O} \frac{v}{X - t} $$

Essentially, the prover has treated the amounts as multiplicities and the types as the underlying set to which the coins belong. This particular example really highlights the power of the reciprocal proof over the polynomial form, since to encode this as a polynomial would make the amounts into exponents, which would substantially complicate the proof. Since this proof is linear in values from different inputs or outputs, it is readily amenable to multiparty proving. The only important caveat is that a transaction using this sort of proof could mint an output of an arbitrary type with zero balance, although this could be mitigated by modifying the range to show $v > 0$. 

\subsection{Sorted Lists}
% TODO better example? 

In the original Bulletproof paper, the authors suggest using a Bulletproof to show that a list $A = \text{sort}(B)$ in zero knowledge wrt to the values being sorted. To do this, they suggest encoding a sorting algorithm directly into an arithmetic circuit and showing that it is satisfied by $A$ and $B$. Since sorting algorithms are provably $\Omega(n \log n)$ in the length of the list, the arithmetic circuit and witness will also be $\Omega(n \log n)$. Using a reciprocal argument, we can prove that $A$ is $B$ sorted with a circuit of size $O(n)$ by first showing that $A$ is a permutation as a multiset of $B$ and then showing that $A$ is sorted. That is, the prover can show

$$ \sum_{i=1}^n \frac{1}{X - A_i} = \sum_{i=1}^n \frac{1}{X - B_i} $$
$$ A_i \leq A_{i+1} $$

This removes the logarithmic factor, thereby reducing the witness size, proving time, and verification time. Unlike the other two examples, this shows how reciprocal arguments can actually achieve asymptotic improvements over derived arithmetic circuits. In this particular case, since we are performing one check and counting each element once, the prover could also use polynomials.

% TODO keep? Don't yet have compelling motivating example
% \subsection{Random Access Associative Memory}
% \subsection{Addition Chains}



\section{Range Proof}

Bulletproofs++ range proofs come in two varieties: binary digit range proofs and arbitrary digit range proofs. Binary digit range proofs work similarly to the other Bulletproof style range proofs but use a norm squared argument to reduce the vector length by half. This reduction in length directly translates into a reduction in proof size by $2$ group elements, as well as a substantial reduction in proving and verification time. Throughout this section, I will use vector length as a proxy for prover and verifier time, since the most complex operations involved in both proving and verification are elliptic curve scalar multiplications, to which vector length is directly proportional. %TODO move?

Arbitrary digit range proofs use a reciprocal argument to show that each digit value belongs to the set of valid digit values. As the size of the range grows, the prover can use larger bases and reduce the total vector length. For example, in the case of $64$ range proofs common in cryptocurrency protocols, the prover can exploit the fact that $2^{64} = 16^{16}$ to use $16$ hexadecimal digits. Arbitrary base range proofs also use the norm argument to further reduce the vector length. Whether an arbitrary base range proof is more efficient than a binary range proof, as measure in proof size and vector length, will depend on the size of the range(s) and the number of ranges to prove.

In both cases, Bulletproof++ range proofs are able to efficiently prove arbitrary ranges, i.e. ranges that are powers of the base. In the binary case, and sometimes for larger bases, for a range $a < v \leq b$ the prover first shifts to show $0 < v - a \leq b - a$ and uses  $\lceil \log_2 (b - a) \rceil$ digits with the final base value reduced so that the maximum representable value is $b - a$. For arbitrary bases in general the situation is more complicated and the prover must sometimes use an additional digit.

\subsection{Binary Norm Proof}

Let $C$ be a commitment to the value $v$ and suppose the prover wants to show $0 \leq v < B$ without loss of generality w.r.t the lower bound. Let $n = \lceil \log_2 B \rceil$ and let the vector $\vec{b}$ be the vector of base values such that $b_{n-1} = B - 2^{n-1}$ and $b_i = 2^i$ for $i=0..n-2$. Note that the maximum representable value, given by the sum of all the base values, is $B-1$ and that this is possible because some values of $v$ have more than one valid digit representation. The prover must then show

$$ \sum_{i=0}^{n-1} d_i b_i = v \qquad d_i^2 = d_i $$

That is, show the digits are bits and that they sum to $v$ with the base values. Using the same technique as Bulletproofs, these independent digit constraints can be combined into a single vector constraint by taking a random linear combination. To do this, given that the digit vector $\vec{d}$ has already been committed, the prover can modify the basis points and exploit the fact that

$$ \com{}{\vec{G}; \vec{v}} = \com{V}{Q^{-1} \vec{G}; Q \vec{v}} $$

Where the diagonal matrix $Q$ satisfies $Q_{ii} = q^{1+1}$. To recover the linear constraint, the public constraint vectors can be multiplied by the inverse of this matrix. Then, to convert the digit constraints into a form that is convenient for a norm proof we can complete the square and show that

$$ \left( d_i - \frac{1}{2} \right)^2 - \frac{1}{4} = 0 $$

Putting all of this together, the prover can show that

\begin{align*}
    \norm{ Q \left(\vec{d} - \frac{1}{2} \vec{1} \right) + Q^{-1} \vec{b} } 
        &= \norm{Q \left(\vec{d} - \frac{1}{2} \vec{1} \right)} + 2 v - \ip{\vec{1}}{\vec{b}} + \norm{Q^{-1} \vec{b}} \\ 
        &= 2 v + \norm{\frac{1}{2} Q \vec{1} - Q^{-1} \vec{b}}
\end{align*}

Where norm term is independent of the committed values. To generalize this to multiple values, we can concatenate all the digit vectors for each value $v_i$ into one large vector $\vec{d}$ and scale each base vector by a distinct power of $x$ and concatenate the results into a single vector $\vec{b}(x)$. Then the base-digit inner product becomes

$$ \ip{\vec{d}}{\vec{b}(x)} = \sum_{i} v_i x^i $$

The value $x$ will be selected after the prover has committed to the $v_i$ and combined digit vector $\vec{d}$. Putting this all together into a protocol, to show that

$$ C_i = s_i H + v_i G \quad s.t. \quad A_i \leq v_i < B_i \quad \text{for} \quad i = 1 \dots k$$

Let $n_i = \ceil{\log_2 (B_i - A_i)}$ and 

$$ \vec{b}_i = \left[ 2^0, 2^1, \dots, 2^{n_i-2}, B_i - 2^{n_i - 1} \right] \qquad \vec{b}(x) = x b_1 \oplus \dots \oplus x^k b_k $$

Then let $\vec{d}_i$ be a, not necessarily unique, binary vector such that $\ip{\vec{d}_i}{\vec{b}_i} = v_i$.

%TODO typeset pretty

\begin{enumerate}
    \item Commit $D = \com{v}{\vec{d}}$
    \item Choose $x, q \leftarrow \mathbb{F}_p$
    \item Invoke norm argument on $P = \com{v}{\norm{\frac{1}{2} Q \vec{1} - Q^{-1} \vec{b}(x)} - \ip{\vec{x}}{\vec{A}}; Q^{-1} \vec{b}(x) -\frac{1}{2} Q \vec{1}} + D + 2 \sum_{i=0}^k x^i C_i$ with the modified basis $\vec{G}' = Q^{-1} \vec{G}$
\end{enumerate}

\subsection{Arbitrary Base Norm Proof}

The arbitrary base norm proof is considerably more complex than the binary case, but can achieve much smaller vectors and smaller overall proofs for many cases including $64$ bit ranges. The basic structure follows straightforwardly from the general structure of the reciprocal argument outlined in the first section, since for a base $b$ range proof we want to show that a collection of digits $\vec{d}$ all belong to a set of valid symbols 

$$d_i \in \{ 0, 1, \dots b-1 \}$$

As alluded to before, there are two cases worth considering for arbitrary base range proofs: the case where each value uses its own set of digit symbols and the case where all the values share a set of digit symbols. The latter case is actually simpler and asymptotically smaller, but for a small number of ranges it often results in larger overall proofs. It also is less general since it forces all the values to be represented in the same base. I will consider the shared digit case first and then the separate digit case.

In both both cases, representing arbitrary ranges is also more complex than in the binary case. Without loss of generality, we will only consider ranges of the form $[0, B)$ since we can always shift to this form. Letting $n = \ceil{\log_b B}$ the three cases are

\subsubsection{$b - 1 | B - 1$}

In a sense, this is the natural generalization of the binary case since $2 - 1 = 1$ divides every range. For arbitrary bases this is obviously not the case, but when it is we can use the base vector $\vec{b}$ 

$$ b_i = b^i \quad for \quad i < n-1 \qquad b_{n-1} = \frac{B - b^{n-1}}{b - 1} $$

Expanding out the maximum representable value, we find

$$\sum_{i=0}^{n-1} (b - 1) b_i = (b^{n-1} - 1) + (B - b^{n-1}) = B - 1$$

\subsubsection{$b^{n-1} < B \leq 2 b^{n-1}$}

When the range is not divisible by $b-1$ but is sufficiently ``close" to the lower power of $b$ we can still use $n$ digits by making the last digit binary. That is $d_{n-1} \in \{ 0, 1 \}$. To see how this works, observe that the subrange spanned by the first $n-1$ digits is $[0, b^{n-1})$. Since this at least half the distance to $B$, we can overlay the same range shifted up by $B - b^{n-1}$ and cover the whole range $[0, B)$. Thus in this case we let 

$$ b_i = b^i \quad \text{for} \quad i < n-1 \qquad b_{n-1} = B - b^{n-1} $$

\subsubsection{$2 b^{n-1} < B < b^n$}

In the final case, we need to add an additional digit increasing the length of $\vec{b}$ to $n+1$. However, we cannot let $b_{n-1} = b^{n-1}$ since this would allow representing values outside the range. Instead, we can choose some value so that the maximum representable value with the first $n$ digits is half way to $B$ and then use another binary digit like the previous case. We can explicitly compute such a value and let

\begin{gather*}
    b_i = b^i \quad for \quad i < n-1 \\
    b_{n-1} = \ceil{\frac{B-1}{2(b - 1)}} - \frac{b^{n-1} - 1}{b - 1} \qquad b_n = B - (b - 1) b_{n-1} - b^{n-1}
\end{gather*}

\subsubsection{Constraints}

The shared and separate digit proofs encode basically the same system of constraints and only really vary in how they are laid out for multiple values. In this section, we will describe the system of constraints for a base $b$ range proof for range $[0, B)$ for one value with a binary digit and generalize this in the following sections. Let $\vec{b}$ be defined as above according it which case we are considering for a given base and bound and define $n$ such that the length of $\vec{b}$ is $n+1$. To recover the case without a binary digit, the protocol can simply drop the bit component of all the vectors.

Instead of using the vector of digits $\vec{d}$ as defined above, we will let $\vec{d}$ be the vector of digits multiplied by the corresponding base value so that

$$ \ip{\vec{d}}{\vec{1}} = v $$

Let $\vec{m}$ be the vector of multiplicities for each digit symbol $0, \dots, b-1$ in the chosen encoding of $v$. As in the reciprocal proof, the prover will commit to $\vec{d}$ and $\vec{m}$ and the verifier will choose a random challenge value $t$. The prover will respond by committing to the vector of reciprocals $\vec{r}$ and both will publicly compute the vector $\vec{c}$ so that

$$ r_i = \frac{b_i}{t b_i - d_i} \qquad c_i = \frac{1}{t - i} $$

Where $d_i$ is the $i$ digit multiplied by $b_i$ as defined before. The prover should also set the bit component of $\vec{r}$ to zero. After committing to $\vec{r}$ the prover must show the following constraints all hold,  excluding the last if the binary digit is not necessary for the given range 

\begin{gather}
    \label{eq:values} \ip{\vec{1}}{\vec{d}} = v \\
    \label{eq:recips} \ip{\vec{1}}{\vec{r}} = \ip{\vec{c}}{\vec{m}} \\ 
    \label{eq:mults}  (t b_i - d_i) r_i = b_i \quad \text{for} \quad i < n \\
    \label{eq:bit}    \left( \frac{b_n}{2} - d_n \right)^2 = \frac{b_n^2}{4} 
\end{gather}

The first constraint checks that the digits sum to the committed value, the second two check that the sum of reciprocals is well constructed and in so doing that all the digits are valid base $b$ digits, and the final constraint checks that the binary digit is valid.

We can extend the constraints to multiple values in the same manner as the binary range proof by multiplying each value by a distinct power of $x$. Since the base values are combined into the digit commitments, we must modify the basis elements in the same way that we do for the multiplicative constraints. This should occur before committing to $\vec{r}$.

\subsubsection{Shared Digits}

In the case where multiple values share the same digit commitments, the vector $\vec{m}$ is defined to be the total number of times that each digit symbol occurs in all the digits for all the values. This vector will be laid out in the linear portion of the combined linear and norm commitment. The vector $\vec{d}$ will be organized as we just described, with the digits for each value concatenated together into one long vector, and $\vec{r}$ will be organized so that the re       ciprocal occurs in the same component as the corresponding digit. All the bit components of $\vec{r}$ will be set to zero as in the single value case.

To show that the constraints are satisfied, we will need to transform the multiplicative constraints into an inner product. In the same manner as the binary range proof, let $Q$ be a diagonal matrix, but now let $Q = U + z V$ where $U_{ii} = q^{i+1}$ for the non-bit components and zero on the bit components, and $V_{ii} = q^{i+1}$ on the bit components and is zero off the bit components. We can then construct the polynomial $\vec{p}(y, z)$ that encodes the constraints in the norm equation as

$$ \vec{p}(y, z) = y Q \vec{r} + y^3 Q^{-1} \vec{1} + y^5 Q (t \vec{b} - \vec{d}) $$
\begin{align*}
    \norm{\vec{p(y, z)}} &= y^2 \norm{U \vec{r}} + 2 y^4 \ip{\vec{r}}{\vec{1}} + y^6 (\norm{Q^{-1} \vec{1}} + 2 \ip{U \vec{r}}{U (t \vec{b} - \vec{d})}) \\
    &+ 2 y^8 \ip{\vec{1}}{t \vec{b} - \vec{d}} + y^{10} \norm{U (t \vec{b} - \vec{d})} + y^{10} z^2 \norm{V (t \vec{b} - \vec{d})}
\end{align*}

Up to public constants and challenges, the $y^4$ term encodes the lefthand side of (\ref{eq:recips}), the $y^6$ term encodes (\ref{eq:mults}), and the $y^8$ encodes (\ref{eq:values}). Expanding the $Q$ values in terms of $z$ we also see that the $z^2 y^{10}$ term encodes (\ref{eq:bit}). To complete (\ref{eq:recips}), the $\vec{m}$ commitment can also be scaled by $y^4$. The remaining terms $y^2$ and $y^{10}$ are error terms, as they depend on committed values and $q$ but do not encode any useful information.

To compensate for the error terms, we will add two additional components to the linear part of the commitment. Since they both depend on $q$, the prover cannot commit to them before or simultaneously with committing to $\vec{r}$. Rather than adding an entirely new commitment, the prover can combine the commitment to these error terms with the blinding commitment. After the verifier has chosen $q$, the prover will choose a blinding value for the scalar component $s_v$ and blinding vectors for the multiplicities, $\vec{s}_m$, and the normed components, $\vec{s}_n$. The the prover will compute the error terms and commit

% TODO this does not properly exclude the z^2 term from a_1
$$ a_0 = \norm{Q \vec{r}} \qquad a_1 = Q (t \vec{b} - \vec{d}) $$
$$ R_1 = \com{LV}{s_v ; \vec{s}_l \oplus a_0 \oplus a_1 ; \vec{s}_n} $$

After this commitment, the prover will choose values for $y$ and $z$. Note that, as written, this will inappropriately scale the $a_0$ and $a_1$ components by the blinding challenge value. To compensate, we will scale these components of the vector by the inverse of the blinding challenge. After the verifier has chosen values for $y$ and $z$, the prover will construct the second blinding commitment, which includes the quadratic term of the scalar and the linear terms of $a_0$ and $a_1$ as polynomials in the blinding challenge. Since we now know $y$, we can pick blinding values for $a_0$ and $a_1$ to make the verification equation holds. That is $s_0$ and $s_1$ so that

$$ s_v = \ip{\vec{s}_l}{y^2 \vec{c}} + s_0 y^2 + s_1 y^{10} + 2 \ip{\vec{s}_n}{\vec{p}(y,z)} $$

Allowing the prover to construct the second blinding commitment

$$ R_2 = \com{LV}{\norm{\vec{s}_n}; \vec{0} \oplus s_0 \oplus s_1; \vec{0}} $$

The verifier can then choose the blinding challenge $e'$, allowing the prover to combine all of the commitments, public constants, and challenges into a single commitment such that if the combined linear norm argument holds, then all the individual range proofs hold as well. TODO write this out explicitly



\subsubsection{Separate Digits}

While it is more efficient for proofs on a large number of values to combine the digit symbols in the linear components, it is sometimes more efficient, as measured both by vector length and proof size, to overlay the multiplicity commitments and the digit commitments on the same basis elements. The reason I call this the ``separate" case is because for multiparty proofs each participant cannot mix their multiplicities with anyone else's digit commitments. 

We need to make two changes in order to overlay the multiplicity commitments and the digit commitments. First, the prover should define the vector $\vec{m}$ to be the concatenation of the digit vectors for each value padded with zeros so that multiplicities and digits from different values do not overlap. The prover will need to commit to $\vec{m}$ and $\vec{d}$ in different commitments. After scaling the basis by $x$ powers, the vector $\vec{c}$ will need to be scaled appropriately to cancel this effect on the $\vec{m}$, but the actual digit symbols appearing in $\vec{r}$ should not be. This will allow summing all the reciprocals at once while prevent the digits of one value from being used in another. Letting the bit components of the new $\vec{m}$ be zero, consider the polynomial

$$ \vec{p}(y,z) = y Q \vec{r} + y^3 Q (t \vec{b} - \vec{d}) - y^5 Q \vec{m} + y^7 Q^{-1} \vec{c} + y^{11} Q^{-1} \vec{1} $$

\begin{align}
    \norm{\vec{p}(y,z)} &= y^2 \norm{U \vec{r}}														\\
                        &+ 2 y^4 \ip{U \vec{r}}{U (e \vec{b} - \vec{d})}							\\
                        &+ y^6 \left(\norm{U (t \vec{b} - \vec{d})} + z^2 \norm{V (t \vec{b} - \vec{d})} - 2 \ip{Q \vec{r}}{Q \vec{m}}\right)		\\
                        &+ 2 y^8 (\ip{\vec{r}}{\vec{c}} - \ip{U (t \vec{b} - \vec{d})}{U \vec{m}})	\\
                        &+ y^{10} (\norm{U \vec{m}} + 2 \ip{t \vec{b} - \vec{d}}{\vec{c}}			\\
                        &+ 2 y^{12} (\ip{\vec{r}}{\vec{1}} - \ip{\vec{m}}{\vec{c}})					\\
                        &+ y^{14} (\norm{U^{-1} \vec{c}} + 2 \ip{t \vec{b} - \vec{d}}{\vec{1}})		\\
                        &- 2 y^{16} \ip{\vec{m}}{\vec{1}}											\\
                        &+ 2 y^{18} \ip{U^{-1} \vec{c}}{U^{-1} \vec{1}}								\\
                        &+ y^{22} \norm{Q^{-1} \vec{1}}
\end{align}

This norm equation encodes constraint (\ref{ref:values}) through (\ref{eq:bit}), up to constants and challenges, on terms $y^{14}$, $y^{12}$, $y^{4}$, and $y^6 z^2$ respectively. The final three terms $y^{16}$, $y^{18}$, and $y^{22}$ can be expressed as constants and challenges since the total number of digits in each number, which $\ip{\vec{m}}{\vec{1}}$ computes, is a public constant. The remaining four terms $y^2$, $y^6$, $y^8$, and $y^{10}$ are error terms that the prover will account for in the linear components in the same manner as combined digit proof.

The rest of the proof continues in the same manner as the combined digit case, the verifier will select the challenge value $q$, the prover will commit to the error terms in the first blinding commitment, the verifier will choose $y$ and $z$, the prover will commit to suitable blinding values for the error terms in the second blinding commitment, the verifier will choose the blinding challenge, and then they will perform the combined linear norm argument on the resulting vector. 

In this proof, the number of linear components is always $4$ and is fully collapsed after two rounds. 



\section{Arithmetic Circuits}

Like the other Bulletproof protocols, we can use the norm and linear innner product arguments to construct a proof system for arbitrary arithmetic circuits. 

\subsection{Public Circuit Proofs}

This proof system is a natural generalization of the Bulletproof style proof systems for arithmetic circuits to the norm and linear product settings. Keeping in mind that there is a large configuration space for these sorts of proof and many opportunities for fine tuning to a specific problem, like in the particular case of range proofs, this proof system will work for general arithmetic circuits presented in the Bulletproof style. That is a proof system for three vectors $\vec{x}$, $\vec{y}$, and $\vec{z}$ and constraint matrices $A$, $B$, $C$, and $D$ with input vector $\vec{v}$ that shows

\begin{gather}
    A \vec{x} + B \vec{y} + C \vec{z} = D \vec{v} \\
    x_i y_i = z_i
\end{gather}

With the additional caveat that multiplication by $D$ from the left is injective so that $\vec{v}$ is recoverable from $D \vec{v}$. To transform this into a system suitable for the combined norm and linear argument, we can use the following identities so as to use the norm argument rather than the inner product argument

\begin{gather}
    (x_i + y_i)^2 - (x_i - y_i)^2 = 4 x_i y_i \\
    (A + B) (\vec{x} + \vec{y}) + (A - B) (\vec{x} - \vec{y}) = 2 \left( A \vec{x} + B \vec{y} \right) 
\end{gather}

This doesn't really reduce the size or complexity of the circuit, so there is no real benefit to using this over an inner product argument. The linear section, however, is suitable for separately multiplying the 

$$ \vec{p}(x, y) = x^3 (Q^{-1} B^{\top} \vec{u} + y Q \vec{a}) + x^5 (y^{-1} Q^{-1} C^{\top} \vec{u} + Q \vec{1} + y^2 Q \vec{c}) + x^7 ( Q^{-1} A^{\top} \vec{u} + y Q \vec{b}) $$


\subsection{Secret Circuits}

In the previous Bulletproof style arithmetic circuit proof systems, the protocol assumes that the circuit is known to the verifier, in the form of the constraint matrices. In some cases, for example when the verifier will be checking many of the same kind of proof, this assumption has a negligible cost. However, in the case that the verifier needs to check many different kinds of proofs with different constraint matrices, and especially when the verifier is decentralized as in the case of a public blockchain, the cost of communicating the circuits themselves is substantial and, depending on the ratio of proofs to circuits and the encoding of circuits, may exceed the cost of communicating the proofs.

Instead of sending the circuit to the verifier explicitly, we can include the circuit itself in the witness and just send a commitment to the circuit. This saves the cost of communicating the circuit and has the added benefit of making the structure of the circuit private from the verifier. This is especially well suited to smart contracts on a public blockchain, as it allows the contract participants to keep the contract itself private while still allowing public verification of its correct execution. 

It is not possible to directly translate Bulletproof verification into a circuit oblivious form, since committing to the constraint matrices directly can be quadratic in the original witness length. Thankfully, this isn't necessary since the constraint matrices are typically sparse, and can have at most a linear number of non-zero entries as a function of the number of circuit operations. However, we are still faced with the problem of specifying, in zero knowledge, which entries of the matrix are non-zero. In a manner analogous to the proof of conservation of money in the first section, we can use a reciprocal proof to encode which constraint each value belongs to in zero knowledge.

To do this, we must first transform the the constraint matrix into a linear sum. Note that the structure of the Bulletproof constraint check $A \vec{x} + B \vec{y} + C \vec{z} = \vec{0}$ is equivalent to ``stacking" the matrices vertically into a single constraint matrix multiplied with the concatenation of the vectors. So, without loss of generality we want to take

$$ \vec{u}^{\top} A \vec{x} = \sum_{k} a_k u_{f(k)} x_{g(k)} $$

Where $a_k = A_{f(k), g(k)}$ is the vector of non-zero entries in $A$. Rather than using a power monomial basis of $u_i = X^i$ to randomize, we can use a reciprocal basis of

$$ u_i = \frac{1}{X - i} $$

Which allows us to encode the mapping $f(k)$ directly as a vector $f_k = f(k)$ and use a reciprocal argument to show that the prover multiplies each term in the linear constraints by the correct randomizer. For convenience, let vector $\vec{c}$ be the reciprocals of the $a_k$ and $\vec{l}$ be the $f(k)$ divided by $a_k$. Letting the reciprocal vector be $\vec{r}_{lin}$, the prover must show

$$ (c_k X - l_k) r_{lin,k} = v_k \qquad \sum_k r_{lin,k} = 0 $$

\subsubsection{Memory}

As well as that the $\vec{v}$ properly encodes $v_k = x_{g(k)}$. While it is possible to do this using multiple linear constraints per $v_k$ value to encode pairwise equalities, a more interesting solution is to use another reciprocal argument to build a ``memory" from which each value can be read and written. This is the final example application of a reciprocal argument from the first section. As noted there, this technique also allows easily building in choice operations to the circuit by allowing the prover to write multiple values to the same address. 

For the vector $\vec{v}$ we will construct the vector $\vec{a}$ of addresses with $a_k = g(k)$. Unlike previous reciprocal arguments, the verifier will choose two random values $(e, r)$ and compute the reciprocal of $e + a_k + r v_k$. Using a single random value and, for example, using $r = e^2$ would allow the prover to choose values that make the associated polynomial factor, which might allow the construction of unsound proofs. Using two random values eliminates this possibility since all the polynomials of the given form are algebraically independent. After the verifier has chosen $(e,r)$ the prover will commit to the vector $\vec{r}_{mem}$ and show

$$ (e + a_k + r v_k) r_{mem,k} = m_k $$

In the case of circuits which do not use choose operations, the vector $\vec{m}$ is the same for all witnesses. In the case that the circuit does use choose operations, the associated $m_k$ values must be specified as part of the witness. To obscure which values are constant or variable, the prover can commit to a vector of binary flags per value and show

$$ (e + a_k + r v_k) r_{mem,k} = m_k b_k + 1 $$

Where $b_k = 0$ for constant multiplicities and $b_k = 1$ for variable multiplicities. While it is convenient to think of $m_k = -1$ being reads and $m_k \geq 0$ being writes, it does not actually matter which values are ``written" and which are ``read" as there is no notion of causality within the constraint system.

\subsubsection{Multiplicative Constraints}

Finally, there are two way to check the multiplicative constraints. The first way is to simply organize the vector $\vec{v}$ into three sections $\vec{v} = \vec{v}_x \oplus \vec{v}_y \oplus \vec{v}_z \oplus \vec{v}_l$ where the prover shows that

$$ v_{x, i} v_{y, i} = v_{z, i} $$

And where $v_l$ are not used in a multiplicative constraint. This is very similar to how other Bulletproof arithmetic circuit protocols worked and it is the one I will elaborate in the protocol section. 

Alternatively, the prover can use a Rank 1 Constraint System (R1CS) like that used by SNARKs \cite{} to encode the multiplications in and the linear constraints together. While I will not use this in the protocol, I think it is worth discussing. A R1CS differs from the Bulletproof style constraint system by taking the componentwise multiplication of the results of the constraint matrix multiplications. That is checking

$$ (A \vec{v})_i (B \vec{v})_i = (C \vec{v})_i $$

It turns out that this structure works well with a reciprocal argument because sums of reciprocals can actually be multiplied. That is

$$ A(x) = \sum_{i} \frac{a_i}{X - r_i} \qquad B(x) = \sum_{i} \frac{b_i}{X - r_i} $$
$$ A(x) B(X) = \sum_{i} \frac{a_i b_i}{(X - r_i)^2} + \frac{q_i}{X - r_i} $$

This follows both from the partial fraction expansion of the product $A(x) B(x)$ since for $i \neq j$

$$\frac{1}{(X - r_i) (X - r_j)} = \frac{1}{r_j - r_i} \left( \frac{1}{X - r_i} - \frac{1}{X - r_j} \right) $$

Or, equivalently, letting $m(r_i) = 0$ and multiplying $\hat{A}(x) = A(x) m(x)$ we can lift the sum of reciprocals into the the quotient ring of polynomials modulo $m(x)$ with $\hat{A}(r_i) = a_i m'(r_i)$. The product of sums of reciprocals lifts to the product of polynomials 

$$ \hat{A}(x) \hat{B}(x) = \hat{C}(x) + m(x) \hat{Q}(x) \qquad \hat{C}(r_i) = a_i b_i m'(r_i)^2 $$

Dividing through by $m(x)^2$ and applying partial fractions yields the prior identity. Using this technique, we can encode the circuit as a R1CS and modify the linear sum of reciprocal to compute four sums of reciprocals $A(x)$, $B(x)$, $C(x)$, and $Q(x)$. Then the R1CS verification is as simple as checking 

$$ A(x) B(x) = \frac{m'(x)}{m(x)} C(x) + Q(x) $$

Since the logarithmic derivative of $m(x)$ is the ``unit" sum of reciprocals 

$$ \frac{m'(x)}{m(x)} = \sum_i \frac{1}{x - r_i} $$

The memory check remains the same. The most significant difference between the R1CS and the Bulletproof constraint system is the quotient sum of reciprocals $Q(x)$. As follows from the equivalence with polynomials modulo $m(x)$, efficiently computing the vector of quotient terms $\vec{q}$ will require an FFT for sufficiently large vectors making the the R1CS version asymptotically slower. As well, using finite fields that do not have suitable roots of unity also increases the asymptotic complexity of the FFT computation, although in practice the cost of elliptic curve operations is still likely to be greater than the cost of computing $\vec{q}$.

\subsubsection{Protocol}

It is more natural to express the oblivious arithmetic circuit protocol using an inner product argument than a norm argument. However, it is possible to directly convert an inner product argument to norm argument using the identities

$$ (a + b)^2 - (a - b)^2 = 4 a b \qquad (a + b) (c + d) - (a - b) (c - d) = 2 (ad + bc) $$

In the case that the $-1$ is a quadratic residue, i.e. the field characteristic is congruent to $1$ mod $4$, the subtraction can be moved inside the square and the inner product can be written a sum of squares of the same length. In the case that $-1$ is not a quadratic residue, the positive and negative terms must be handled separately.

Since the circuit is not already known to the verifier, the prover must commit to the vectors $\vec{c}$, $\vec{a}$, $\vec{l}$, and $\vec{f}$ which together specify the circuit. The prover will also commit to the witness vectors $\vec{v}$ and $\vec{m}$. In practice, the first set of commitments specifying the circuit is likely to be shared first and the second set when the proof is actually constructed. This allows the circuit to be reused among multiple proofs. For example, the circuit commitments would be published when a smart contract is created and would be reused anytime someone interacted with the smart contract.

After committing to the circuit and the witness, the verifier will choose two random challenge values $e$ and $r$, and the prover will commit to the reciprocal vectors $\vec{r}_{\text{mem}}$ and $\vec{v}_{\text{lin}}$. Letting the vector $\vec{v}$ be defined such that it splits into the four subvectors $\vec{v} = \vec{v}_x \oplus \vec{v}_y \oplus \vec{v}_z \oplus \vec{v}_l$ the prover must show

\begin{gather}
    \label{eq:mem} (e + a_i + r v_i) r_{\text{mem}, i} = 1 + f_i m_i	\\
    \label{eq:lin} (e c_i + l_i) r_{\text{lin},i} = v_i					\\
    \ip{\vec{r}_{\text{mem}}}{\vec{1}} = 0			                 	\\
    \ip{\vec{r}_{\text{lin}}}{\vec{1}} = 0								\\
    v_{x,i} v_{y,i} = v_{z,i}
\end{gather}

The first four constraints are just two reciprocal arguments, which we can translate into polynomials in more or less the same way as the earlier protocols. To simplify the handling of the final constraint, we can commit to the variable assignment $\vec{v}$ in two separate commitments. 



There are many more error terms in this protocol and we can mitigate this in two ways. The protocol can either extend the commitments so that, for example, $\vec{v}$ and $\vec{m}$ lie over different bases or the protocol can introduce more rounds of challenge and response. These will increase verification time and proof size respectively, but depending on the use case may a different tradeoff may be preferable. 



As with all of the prior protocols, we will convert the systems of multiplicative constraints into inner products by scaling each by an independent power of a random value $q$ and organize the commitments into a pair of polynomials whose multiplication encodes the constraints. The first two constraints, represented as four inner products, fit neatly into this paradigm with

$$ \vec{l}(x) = x Q \vec{1} + x^3 q^n Q \vec{m} + x^5 Q \vec{r}_{\text{lin}} + x^7 q^{n} Q \vec{r}_{\text{mem}} $$
$$ \vec{r}(x) = x Q (e + \vec{a} + r \vec{v}) + x^3 (e \vec{c} + \vec{l}) + x^5 \vec{f} + x^7 Q \vec{v} $$

The multiplicative constraints are more complex, since the variable assignment vector is given as a single commitment. 


Earlier Bulletproof arithmetic circuit protocols, including+ the previous protocol, assume that the circuit itself is known to the verifier. This is a reasonable assumption when the same kind of proof must be verifier often, for example range proofs in a cryptocurrency. However, when the particular circuit is not known in advance or is one of a large number of possible circuits, it becomes increasingly expensive to share and keep track of all the different circuits. This is especially true in the context of a blockchain, since these circuits must be included on chain at verification time at the latest. Depending on how frequently each circuit is used in a proof, recording the circuits may occupy more space than the proofs themselves.

Ideally, we would prefer that the prover be able to distribute a commitment to the circuit to prove and use the commitment in the proof. This eliminates virtually the entire overhead of distributing the circuit and has the added benefit of homogenizing the proofs themselves and increasing privacy. Using circuit commitments as a smart contract allow the parties to the contract to avoid publishing the details of the contract at all.

TODO discuss SNARKs/PLONK/etc. 

In its current form, Bulletproof arithmetic circuit verification requires multiplication of the matrices $A$, $B$, and $C$ by random public vectors $\vec{u}$ and the committed vectors. Translating this directly to an arithmetic circuit is awkward, especially so if we use a power monomial basis to generate random $\vec{u}$. To make the problem simpler, we can first note that the matrices will typically be very sparse since each non-zero entry corresponds, roughly, to one addition operation in the original circuit. We can then write the matrix product out as

$$ \vec{u}^{\top} A \vec{x} = \sum_k a_k u_{f(k)} x_{g(k)} $$

Where the $k$ ranges over all the non-zero elements of the matrix such that $A_{f(k),g(k)} = a_k$. Now, the vector $\vec{u}$ is used purely for the purposes of randomizing the linear constraints. Instead of using a power basis, the proof can use a reciprocal basis without affecting the ultimate security of the protocol. The reciprocal basis in turn allows us to embed the mapping of $u_i$ elements, which must be kept secret from the verifier, via the particular monomials used. That is, letting

$$ u_i = \frac{1}{X - i} $$

%TODO name collision with multiplicity vector
We can just commit to the vector $n_k = f(k)$ and use a reciprocal argument on these indices. Note that the linear constraints involve multiplying all the constraint matrices by their associated vector and summing the result, which is really equivalent to stacking the constraint matrices into one big matrix $M$ multiplied by $\vec{a} \oplus \vec{b} \oplus \vec{c}$. Applying the same technique to the combined constraints we can define vectors $\vec{m}$, $\vec{f}$, and $\vec{v}$ encoding the $k$ indexed terms so that

$$ \vec{u}^{\top} M (\vec{a} \oplus \vec{b} \oplus \vec{c}) = \sum_k \frac{m_k v_k}{X - n_k} $$
$$ g_k = g(k) \qquad v_k = (\vec{a} \oplus \vec{b} \oplus \vec{c})_{g(k)} \qquad m_k = M_{f(k), g(k)} $$

This faithfully encodes the linear constraints up to the checks that $v_k$ values which come from the same committed values really are equal. While it is possible to modify this system so that, for example, all the $v_k$ are included in multiple linear constraints to check that they are also equal, this is expensive and can be avoided. To avoid the extra constraint, we can use a variant of the linear argument to build a sort of addressable ``memory" in the proof and use this to look up the $v_k$ value be address.

Let the vector $\vec{g}$ be the addresses of the $v_k$ such that $g_k = g(k)$ and let $\vec{x}$ be the 

\newpage
%$$ \vec{p}(y, z) = y Q \vec{r} - y^3 Q \vec{m} + y^5 Q^{-1} \vec{c} + y^7 Q^{-1} \vec{1} + y^{13} Q (e \vec{b} - \vec{d}) $$
%
%Taking the norm and expanding we find
%
%\begin{align}
%    \norm{\vec{p}(y, z)} &= y^2 \norm{Q \vec{r}}														\\
%                         &+ 2 y^4 \ip{Q \vec{r}}{Q \vec{m}}												\\
%                         &+ y^6 (\norm{\vec{m}} + 2 \ip{\vec{r}}{\vec{c}})								\\
%                         &+ 2 y^8 (\ip{\vec{r}}{\vec{1}} - \ip{\vec{c}}{\vec{m}})						\\
%                         &+ y^{10} (\norm{\vec{c}} - 2 \ip{\vec{m}}{\vec{1}})							\\
%                         &+ 2 y^{12} \ip{\vec{c}}{\vec{1}}												\\
%                         &+ y^{14} (\norm{Q^{-1} \vec{1}} + 2 \ip{Q \vec{r}}{Q (e \vec{b} - \vec{d})})	\\
%                         &- 2 y^{16} \ip{Q \vec{m}}{Q (e \vec{b} - \vec{d})}							\\
%                         &+ 2 y^{18} \ip{\vec{c}}{e \vec{b} - \vec{d}}									\\
%                         &+ 2 y^{20} \ip{\vec{1}}{e \vec{b} - \vec{d}}									\\
%                         &+ y^{26} \norm{Q (e \vec{b} - \vec{d})}										
%                         % &+ z^2 y^{26} bit
%\end{align}
%
%The constraints (\ref{ref:values}) through (\ref{eq:bit}) are encoded on terms $y^{20}$, $y^8$, $y^{14}$, and $z^2 y^{26}$ up to constants. The $y^{12}$ is comprised entirely of constants and challenges, and the $y^{10}$ term can be simplified to constants and challenges because the number of digits is a publicly known quantity for each value. The rest of the terms, $y^2$, $y^4$, $y^6$, $y^16$, $y^{18}$, and $y^{26}$, are error terms. However, since $y^18$ does not depend on $q$, we can fit that one in the scalar component of the $r$ commitment and scale the scalar component by $y^17$. This puts the scalar component of the $\vec{m}$ commitment on the $y^{20}$ term, but this cannot interfere with the value commitments since the $\vec{m}$ commitment precedes the selection of $x$. 
%
%To accommodate the five error terms, the prover will add five linear terms where the combined digit proof added two. The rest of the protocol proceeds in more or less the same way, with the verifier choosing $q$, the prover committing to the error terms in the first blinding commitment, the verifier choosing $y$ and $z$, the prover committing to the blinding terms for the error components such that the verification equation holds and the in the second blinding commitment, the verifier choosing the blinding challenge, and finally running the combined linear norm proof on the resulting blinded vector.

This variant will result in shorter vector sizes for the range proofs likely to occur in practice for cryptocurrencies. 





As before, the matrix $Q$ is diagonal with $Q_{ii} = q^{i+1}$ for $i < n$, but in this case is modified so that $Q_{nn} = y q^{n+1}$. Expanding out $\norm{p(x,y)}$, the constraints are encoded as: (\ref{eq:values}) on $x^{16}$, (\ref{eq:recips}) on $x^8$, (\ref{eq:mults}) on $x^{18}$, and $\ref{eq:bit}$ on $y^2 x^{26}$. The fact that $y^2 x^{10}$ and $y^2 x^{14}$ vanish proves that the bit components of $\vec{r}$ and $\vec{m}$ are zero.

The first three terms $x^2$, $x^4$, and $x^6$ are all error terms. The next term $x^8$ encodes (\ref{eq:recips}) and should vanish if the constraint is satisfied. The next two terms $x^{10}$ and $x^{12}$ are both computable from public constants and challenges. In the latter case, we can simplify $\ip{\vec{m}}{\vec{1}}$ because the number of digits is a public constant. The next term $x^{14}$ encodes (\ref{eq:mults}) plus a function of public constants and challenges. The following two terms $x^{16}$ and $x^{18}$ are error terms, but the latter does not depend on $q$, which allows the prover to fit it into the scalar component of the $\vec{r}$ commitment. The final three terms encode (\ref{eq:values}) on $x^{20}$, an error term on $x^{26}$, and the bit constraint ($\ref{eq:bit}$) on $y^2 x^26$.

There are other possible configurations to encode these constraints, but as far as I can tell they all involve at least five error terms and one $q$ independent error term. It should also be noted that scaling the scalar component by $x^{17}$ places the $\vec{m}$ scalar commitment  at $x^{18}$, which could allow it to interfere with (\ref{eq:values}). This will be mitigated by the technique used to prove multiple values simultaneously.

Once the prover has committed to $\vec{r}$, the verifier can select $q$ uniformly at random, which will allow the verifier to actually compute the error terms. Let the vector $\vec{l}$ denote the five error terms. To save space, the prover will combine the commitment to $\vec{l}$ with the first blinding commitment. That is, the prover will choose a uniformly random vectors $\vec{s}_n$ to blind the norm components and a uniformly random scalar $s_s$ to blind the scalar. 

After the prover has committed to this, the verifier will choose values for $x$ and $y$, allowing the prover to complete the second blinding commitment, with $\norm{\vec{s}_n}$ in the scalar and a uniformly random blinding vector $\vec{s}_l$ for the linear components that satisfying

$$ s_s = \ip{\vec{s}_l}{\left[ x^2, x^4, x^6, x^{16}, x^{26} \right]} + 2 \ip{\vec{s_n}}{\vec{p}(x,y)} $$

At this point, the prover will recursively engage the linear combination and norm argument to show that the proof is correct. Note that, the linear components must be inversely scaled by the challenge value. If the length of the normed vector is greater than $8$, the result of the recursive argument will compress the entire linear constraint into one scalar value. Whether this technique is preferable to the binary technique depends on the total length of the normed vector. That is to say, the number of individual ranges and the size of these ranges. However, this technique achieves both shorter vectors and smaller proofs for one $64$ bit range proof or two combined $32$ bit range proofs.



Unfortunately, these equations do not admit quite as simple an encoding into a norm equation as the binary range proof. We can still construct an equation where some of the terms encode these constraints and some of the terms do not depend on committed values, but we will still be left with some terms that do not encode constraints but do depend on committed values, which we can call ``error" terms. Using the combined linear and norm argument, we can compensate for the error terms without sending additional commitments thus keeping the overall proof small.

Starting with the sum of reciprocals equation (\ref{eq:recips}), we can arrange the four terms $\vec{c}$, $\vec{1}$, $\vec{m}$, and $\vec{r}$ so that the desired products occur in the same term in a polynomial norm equation. As in the binary range proof, we will scale all the commitments by a diagonal matrix $Q$ and the public vectors by $Q^{-1}$.

$$ \left[ x^{8} \right] \norm{ x Q^{-1} \vec{c} + x^3 Q^{-1} \vec{1} + x^5 Q \vec{r} - x^7 Q \vec{m} } = \ip{\vec{1}}{\vec{r}} - \ip{\vec{c}}{\vec{m}} $$

This introduces six additional terms, two of which are expressible using public constants and challenges and four of which depend on committed values. Notice that the while the $x^6$ coefficient $2 \ip{\vec{c}}{\vec{r}} + \norm{Q \vec{1}}$ contains both committed values and $Q$, they are not multiplied together. This means the first term is computable by the prover at commitment time for $\vec{r}$ and the prover can put it in the scalar component of the $\vec{r}$ commitment. The second term is computable publicly. This will, eventually, require multiplying the scalar components by the inverse of $x$, which we must ensure does not allow the scalar terms of any other commitments to interfere with the constraint terms.

We can add the digit vector on the $x^{13}$ term, which has the nice property of combining two of the error terms on the $x^{14}$ coefficient and ensuring that equations (\ref{eq:values}), (\ref{eq:mults}), and (\ref{eq:bit}) are, almost, free from interference. The bit constraint is unfortunately combined with an error term on $x^{26}$, but we can remedy this by modifying the $Q$ matrix so that $Q_{nn} = y q^{n+1}$, which places the bit constraint on the term $x^{26} y^2$ and the error on $x^{26}$. Since the $n$ components of all the other commitments are zero, there should be no other $y$ terms. This leaves the polynomial

$$ \vec{p}(x, y) = x Q^{-1} \vec{c} + x^3 Q^{-1} \vec{1} - x^5 Q \vec{r} - x^7 Q \vec{m} + x^{13} (e \vec{b} - \vec{d}) $$

Taking the norm of $\vec{p}(x,y)$, the constraint (\ref{eq:values}) is encoded on $x^{16}$, (\ref{eq:recips}) on $x^6$, (\ref{eq:mults}) on $x^{18}$, and (\ref{eq:bit}) on $x^{26} y^2$ with five error terms $x^{10}$, $x^{12}$, $x^{14}$, $x^{20}$, and $x^{26}$ with the commitment dependent part of the error term $x^6$ stored in the scalar of the commitment to $\vec{r}$. 

Once the prover has committed to $\vec{r}$, the verifier can choose a value $q$. The prover can simplify this polynomial to a polynomial in $x$ and $y$ and determine the correct scalar values for the error terms. These scalars will be stored in a vector $\vec{l}$ that the prover will ultimately use as the linear part of the norm and linear argument. To save space, the prover can actually combine the commitment to this vector with the blinding procedure. Note, while this will scale the linear terms by the blinding variable, the protocol can simply factor this into the linear constraint or equivalently modify the basis elements.

So, the prover will choose a uniformly random blinding vector $\vec{s_n}$ for the normed components and a scalar $\vec{s_s}$ for the scalar component. The prover will compute the error terms, given $q$, to produce $\vec{l}$ and commit to all of these values at once in

$$ N_1 = \com{lv}{s_s; \vec{l}; \vec{s}_n} $$

At this point the verifier will choose values for $x$ and $y$ uniformly at random and the prover will choose a blinding vector for the linear terms that preserves the final verification procedure and construct the second blinding value $N_2$. That is, $s_l$ such that

$$ \ip{s_l}{\left[ x^{10}, x^{12}, x^{14}, x^{20}, x^{26} \right]} = s_s - 2 \ip{\vec{s}_n}{\vec{p}(x, y)} $$
$$ N_2 = \com{lv}{\norm{\vec{s}_n}; \vec{s}_l; \vec{0}} $$

The verifier can select one final challenge value $a$ to complete the blinding procedure. To complete the range proof, the prover will engage the recursive linear and norm argument on the linear combination of commitments dictated by the challenge values

$$ P + x^5 R - x^7 M + x^{13} D - x^{17} V + a N_1 + a^2 N_2 = \com{lv}{u; \vec{v}; \vec{w}} $$

Such that

$$ u = \frac{1}{a} \ip{\vec{v}}{\left[ x^{10}, x^{12}, x^{14}, x^{20}, x^{26} \right]} + \norm{w} $$

Where $P$ constructed from all the publicly computable values as

$$ P = \com{lv}{\norm{x Q^{-1} \vec{c}+ x^3 Q^{-1} \vec{1}} + e x^{16} \ip{\vec{1}}{\vec{b}} + x^{18} \norm{Q \vec{1}} + x^{26} y^2 \frac{b_n}{4}; \vec{0}; x \vec{c} + x^3 \vec{1}} $$


\section{Proofs}

The four important properties that Bulletproof style zero knowledge proofs must show hold for a given protocol are Completeness, Soundness, Special Honest Verifier Zero Knowledge (SHVZK), and Computation Witness Extended Emulation. The first two will depend in a nontrivial manner on exactly what the protocol is designed to actually prove, but the latter two do not. Observing the proofs in Bulletproofs and Bulletproofs+, the SHVZK and CWEE proofs are fairly similar and proceed via a general process: each stage of the proof has some inputs, generates challenges, combines the inputs according to the challenges, and provides more commitments for the next stage. Here I will attempt to generalize this kind of proof and provide general proofs that SHVZK and CWEE follow assuming the challenges are chosen appropriately.

Firstly, we will assume that every commitment given is distributed uniformly at random. This is straightforwardly accomplished by blinding every commitment. We will also assume that every scalar response, from the prover to the verifier, is also distributed uniformly at random. This is also accomplished by blinding. In each stage of the proof, the prover will have a vector of commitments $\vec{C}_i$ and a vector of scalars $\vec{x}_i$. The verifier will choose a challenge matrix $M$ with which to transform the commitments which may depend on all the public scalars up until this point in the proof, that is previous challenge matrices and scalar responses. The prover will respond with a vector of response commitments $\vec{R}_i$ and response scalars $\vec{y}_i$ as well as open some of the commitments $M_i \vec{C}_i$. Letting $\vec{A}_i$ be the commitment that the prover will open after the challenge, the prover will update for the next round as

$$ M_i \vec{C}_i = \vec{A}_i \oplus \vec{B}_i $$
$$ \vec{C}_{i+1} = \vec{R}_i \oplus \vec{B}_i \qquad \vec{x}_{i+1} = \vec{x}_i \oplus \vec{y}_i $$ 

In the final round, the prover will provide no additional response commitments and open all of $M_i \vec{C}_i$. That is, $\vec{B}_i$ will be empty. To show that SHVZK and CWEE hold, all the prover must show is that the challenge matrices have a particular form. Firstly, without loss of generality, the challenge matrix distribution must have linearly independent rows with overwhelming probability, since otherwise the linearly dependent rows could be consolidated into a single component. Then, the matrix must have more columns than rows. Each row of the matrix must be linearly independent of any vector computable from public scalars up until this point in the protocol with overwhelming probability. Finally, the rows of two different challenge matrices must be have maximal rank with overwhelming probability. The class of such matrices that occurs most in practice is matrices of rational functions in some challenge values such that every component within a row is linearly independent as a rational function from the other components in the row and so that every row is linearly independent of the other rows.

At this point, we can show that both SHVZK and CWEE hold assuming that the challenge matrix for each round is drawn from an appropriate distribution. For the former, we can show that a valid proof transcript is distributed uniformly at random by choosing random scalar responses, choosing a random challenge matrix for each round according to the distribution for the round, and then rewriting all the openings as functions of the response commitments and the initial commitments. Since the challenge matrix for every round is underdetermined, the resulting matrix is also underdetermined. We can then sample a valid responses vector uniformly at random by choosing a number of responses uniformly at random equal to the dimension of the null space, i.e. total number of responses minus number of openings with overwhelming probability, and then solve for the remaining openings as a function of the challenge matrices, scalar responses, and random responses.

To show CWEE, we being at the final round and sample challenge matrices for until we have at least a fully determined linear system over $\vec{R}_{n-1}$ and $\vec{B}_{n-1}$. Given the openings for the various challenge matrices, we can then solve the openings of $\vec{R}_{n-1}$ and $\vec{B}_{n-1}$. We then add, if any, the openings for the $\vec{A}_{n-1}$ so that we have a valid opening for all of $\vec{C}_{n-1}$ and repeat the procedure. By induction, we will eventually arrive at the initial commitment vector $\vec{C}_0$ and extract the witness. By the generalized forking lemma, if the challenge matrix for round $i$ has rank $r_i$ and columns $c_i$ with overwhelming probability, this will involve $\prod_i (c_i - r_i + 1)$ transcripts. As long as this is polynomial in the security parameter, CWEE follows from the generalized forking lemma.

These proof techniques have the nice property that they are compositional. That is, given a protocol composed of several sub protocols, the subprotocols can be shown to satisfy SHVZK and CWEE independently and then reused as opaque rounds in the larger protocol. For example, the inner product argument is used in such a way by Bulletproof type protocols.

\subsubsection{Correctness and Soundness}

Correctness and Soundness can also be derived more mechanically, to an extent, as they typically follow directly from the Schwartz-Zippel lemma. The essential criteria is that the prover must commit to enough information to completely determine a polynomial before choosing the challenge point. If the protocol satisfies this property, then the protocol proves with overwhelming probability that the coefficients of the polynomial are identically zero. Composing this technique, i.e. an inner product or norm argument, allows the prover to show that some non-trivial algebraic relations hold among the committed values. 

This technique can be composed which allows the construction of more complex protocols to show 


Soundness may also require other variants of this argument, for example in the reciprocal argument. The key is that 

\newpage





This introduces three more error terms $x^{18}$, $x^{20}$, and $x^{30}$. The final term also encodes the bit constraint, which is a problem. To separate the bit constraint out, the prover can modify $Q_{nn} = y q^{n+1}$ so that the $x^{30}$ term is an error term but the $x^{30} y^2$ encodes the bit constraint, up to a constant. Since all the other commitments are zero in the $n$ component, there should be no other $y$ terms. 

Note also that the $x^18$ error term is $\ip{\vec{c}}{e \vec{b} - \vec{d}}$. Because the term does not depend on $q$, it is known to the prover at commitment time for $\vec{r}$. The prover can put this term in the scalar part of the commitment to $\vec{r}$ which reduces the number of error terms necessary to compensate for to $5$. Later on, the prover will multiply the scalar component of all the commitments by $x^{18 - 7} = x^{11}$, which a quick

For the sake of generality, we will consider the final case when describing the protocol. To recover the simpler cases, the prover need only use an appropriate $\vec{b}$ vector and drop the bit element from all the vectors if applicable. For simplicity, we can assume also that $b \leq n$ and that all vectors are length $n + 1$, with $n = \lceil \log_b B \rceil$ as before.

Let the vector $\vec{m}$ be the vector of multiplicities for each symbol $0, ..,b-1$ padded with zeros as necessary. Let the vector $\vec{d}$ be the vector of digits multiplied by the base value $\vec{b}$, so that summing $d_i$ gives $v$. Using the reciprocal argument from before, the prover will commit to these vectors and then generate the vector $\vec{r}$ of digit reciprocals using the challenge value, but with the final $r_n = b_n - d_n$. Let $\vec{c}$ be the, public, vector of reciprocals for each symbol. The prover wants to show 

$$ \sum_{i=0}^{n} d_i = v $$
$$ \ip{\vec{c}}{\vec{m}} = \ip{\vec{1}}{\vec{r}} $$
$$ (e b_i - d_i) r_i = b_i $$
$$ (d_n - \frac{b_n}{2})^2 - \frac{b_n}{4} = 0 $$

Notice that, in effect, we have multiplied the usual reciprocal equation by the corresponding digit value, but the $r_i$ value is the same. Using the same technique of multiplying by the matrix $Q$ as before we can transform the multiplicative constraints into an inner product equation.

This system is considerably more complex than the binary case, but we can still construct a norm equation that encodes it with some work. Firstly, we note that the second linear equation, encoding the sum of reciprocals, can be naturally encoded as the vanishing of the $x^8$ term in the following norm squared equation.

$$ | x Q^{-1} \vec{1} + x^3 Q \vec{c} + x^5 Q^{-1} \vec{m} + x^7 Q \vec{r} |^2 $$

Some of these terms are expressible in terms of constants and $q$, like $x^2$ and $x^4$, while others depend on committed values but do not encode constraints, which we can refer to as ``error" terms. The highest order term from this equation is $x^{14}$, so we can introduce the multiplicative constraints and digit summation constraints by using an $x$ term that will not interfere with the lower order terms, $x^{15} Q (e \vec{b} - \vec{d})$. This places the multiplicative constraints on $x^{22}$, the final constraint for the binary digit check on $x^{30}$, and adds three additional error terms.

To separate out the bit constraint from the rest of $|e \vec{b} - \vec{d}|^2$, the prover can modify the matrix $Q$ so that the $n$ component of the vectors is also multiplied by a new variable $y$. Finally, note that the error term $x^{18} \ip{\vec{c}}{e \vec{b} - \vec{d}}$ is independent of the values $Q$ and $y$. This allows the prover to fit this term into the scalar part of the $\vec{r}$ commitment. 

After committing to $\vec{r}$, and the $x^{18}$ error term, the verifier can select the challenge value for $Q$. This allows simplifying the norm equation up to the values of $x$ and $y$. In particular, the prover can evaluate all five remaining error terms. Using the combined norm and linear proof from before, the prover can add linear terms to compensate for the error terms at the cost of one, or two depending on the size of the range, additional scalars in the final proof. In fact it is actually possible to combine the commitment to these error terms with the blinding of the proof so that the prover need not send any additional commitments, besides those for blinding.

After choosing $q$, the prover will send a commitment $N_1$ to the vector $\vec{e}$ of error terms, the vector $\vec{s_v}$ of blinding values for the norm component commitments, and a blinding scalar $s_s$ for the scalar term. The blinding values should be chosen uniformly at random. Then, the verifier will sample $x$ and $y$ uniformly at random, and the prover will send a commitment $N_2$ committing to the norm $|\vec{s_v}|^2$ in the scalar, nothing in the norm components, and a randomly chosen $\vec{s_l}$ for the linear components so that the inner product with the $x$ and $y$ monomials balances the equation. Finally, the verifier will choose the challenge $z$ to combine the blinding terms and the prover will separately scale the linear basis elements by $z$. 

\begin{enumerate}
    \item Commit $D = \com{v}{\vec{d}}$, $M = \com{v}{\vec{m}}$
    \item Choose $e \leftarrow \mathbb{F}_p$
    \item Commit $R = \com{v}{\vec{r}}$
    \item Choose $y, q \leftarrow \mathbb{F}_p$
    \item Commit $N_1 = \com{lv}{s_s; \vec{l}; \vec{s}_v}$
    \item Choose $x, z \leftarrow \mathbb{F}_p$
    \item Commit $N_2 = \com{lv}{|s_v|^2; \vec{b}; \vec{0}}$
    \item Choose $a \leftarrow \mathbb{F}_p$
    \item Check combined Linear and Norm proof $(P + x^5 M + x^7 R + x^{15} D) + a N_1 + a^2 N_2$ with modified basis
\end{enumerate}

While this procedure might seem somewhat convoluted, it allows us to save two group elements at the cost of one scalar for $64$ bit range proofs, while simultaneously reducing the combined linear and norm vectors by a factor of $64/21 > 3$ as compared to the binary Bulletproof++. While much of the focus on Bulletproof range proof research has been focused on reducing proof size, given the highly space constrained nature of decentralized blockchains, improving the prover and verifier time is also important in practice. While batching is an effective strategy for verification of many separate range proofs, range proofs for multiple values can increase in verification and prover complexity quickly.





\subsection{Arbitrary Range}

While many range proofs may lie over ranges that are conveniently expressible as $0 \leq v < b^n$ this is obviously not true in general. For a boundary $b^(n-1) < B < b^n$, the prover can use two range proofs to show that $0 \leq v < b^n$ and $0 \leq B - v - 1 < b^n$, but we can do better by using a different basis than $b^i$.

\subsubsection{$b-1 | B-1$}

Assuming we have digits $0$ through $b-1$ there are several cases to consider. The simplest case is $b-1 | B-1$, which is always true for binary. Recall that $b^k - 1 = (b - 1)(\sum_{i=0}^{k-1} b^i)$, so using the basis $b_i = b^i$ for $i=0,..,n-2$ the prover can represent all the values $0 \leq v < b^{n-1} - 1$ when digit values are drawn from $0$ to $b-1$. Using the basis element $b^{n-1}$, we get a range that is too large, so instead the prover can use

$$ b_{n-1} = \frac{1}{b - 1}{(B - 1) - (b^{n-1} - 1)} $$

This ensures that the largest representable base $b$ linear combination of the $b_i$ is $B$, at the expense of some values having multiple valid representations. This does not affect the range proof and allows proving $0 \leq v < B$ with a witness of length $\lceil \log_b B \rceil$, which is half the length of the double range proof protocol described earlier.

\subsubsection{$B/2 \leq b^{n-1} - 1 < B$} 

For binary digits $b = 2$ so $b - 1 = 1$ and the divisibility condition is always satisfied. However, for bases greater than $2$ and arbitrary ranges, this will not typically be true. In the case that $B$ is less than double $b^{n-1}$, the prover can instead choose a value $B/2 < A < b^{n-1}$ that is divisible by $b-1$ and set $b_{n-2} = \frac{1}{b - 1} (A - b^{n-2} + 1)$. This ensures that

$$ (b-1) \sum_{i=0}^{n-2} b_i = A $$

Since $A > B/2$, the prover can then set the final basis element to be $b_{n-1} = B - A - 1$ and, instead of using one of the $b$ symbols, the prover can use a bit. This ensures that the maximum representable linear combination of the basis elements is $B-1$, and so this is a valid basis for a $0 \leq v < B$ range proof.

\subsubsection{$b^{n-1} - 1 < B/2$}

The case where $B/2$ is not representable using $n-2$ digits can be handled similarly, except it requires an additional basis element to handle the bit. That is, the prover chooses some $B/2 < A$ and $d-1 | A$, but must replace the $n-1$ digit with $b_{n-1} = \frac{1}{b - 1} (A - b^{n-1} + 1)$. Then the prover will set $b_n = B - A - 1$. This increases the number of digits necessary by 

\section{Reciprocal Argument}

Both Bulletproof and Bulletproof+ range proofs are designed to show that some committed $v$ satisfies $0 \leq v < 2^n$ by decomposing $v$ into binary, showing that the inner product of the binary digits with powers of $2$ equals $v$ and that each digit is either $0$ or $1$. This final check is phrased as $b^2 - b = (b-1) b = 0$ and requires one multiplication and one linear constraint. Attempting to generalize to other bases, we find that the number of multiplications grows linearly in the base, which is undesirable since $\frac{b-1}{\log b} > \frac{1}{\log 2}$ for all integer $n$. 

However, rather than using a polynomial, the digit check can be phrased a set membership problem over a, presumably, small set of digit values. Happily, it turns out that it is possible to check this, that the digits are all members of a particular set, using one multiplication per digit and one multiplication per element of the set. Letting $d_i$ by the $i$th base $b$ digit of $v$ and $m_j$ by the number of digits $d_i = j$, we can check the equality of rational functions at a random challenge point

$$ \sum_{i=0}^{n-1} \frac{1}{X + d_i} = \sum_{j=0}^{b-1} \frac{m_j}{X + j} $$

To see why this works, clear the denominators of both sides to transform the equation into one of polynomials. By the Schwartz-Zippel lemma, this equality holds as polynomials with overwhelming probability if it holds for a uniformly random challenge value $X=e$. If this equality of polynomials holds, then the equality of rational functions holds, or the challenge is a root of both denominators, not necessarily mutually exclusively in general for rational functions. Since the denominators are of degree $\max(n, d)$ which is a negligible function in the security parameter by assumption, the probability of random $X$ being a root of both is negligible. Alternatively, since there is a ``maximal" RHS denominator we can just discard $0 \leq e < d$ values to ensure that $e$ can never be a root of both denominators. However, this is not necessary and is not possible in the general case where both denominators may depend on committed values.

Now, instead of $(b-1) n$ the number of multiplications grows like $n + b$. For binary, the original check is still superior since it only requires one multiplication per bit of range, but as $b$ grows the advantage grows as well. For example, a $64$ bit range proof, which is common in practice in cryptocurrencies, can use $16$ hexadecimal, base $16$, digits to get $32$ total multiplications. In fact, using the norm proof, these multiplications can be done over vectors of length $16$ instead of length $128$ in the case of Bulletproofs.

\section{Range Proofs}

To show that $0 \leq v < b^n$ using the reciprocal argument, we need to first commit to vectors $\vec{d}$ and $\vec{m}$ of the base $b$ digits and the digit multiplicities respectively as defined before. Then the verifier will select a random challenge point $e$ to evaluate the equality. This allows the prover to construct the vectors $\vec{r}$ of reciprocals. For convenience, let $v_i(n) = 1$ if $0 \leq i < n$ and $0$ otherwise, extend all vectors to $\max(d, n)$, and define the public constraint vectors

$$ a_i = v_i(n) $$
$$ b_i = b^i v_i(n) $$
$$ c_i = \frac{v_i(d)}{e + i} $$

The prover must the show that the following equations hold. 

$$ \ip{\vec{a}}{\vec{r}} = \ip{\vec{c}}{\vec{m}} $$
$$ v = \ip{\vec{b}}{\vec{d}} $$
$$ (e + d_i) r_i = 1 $$

To help in laying out these constraints into an inner product, notice that $\ip{\vec{m}}{\vec{a}} = n$ by construction, since each $m_i$ is the number of occurrences of the digit $i$ and the total number of digits is public knowledge. Then, as in Bulletproofs(+), we can take a random linear combination of the linear constraints to transform them into a single linear constraint. This is useful for a later generalization of these range proofs to arbitrary ranges. Considering the following inner product

$$ \ip{\vec{b} x + \vec{m} x^3 + \vec{r} x^5}{\vec{a} x^3 + \vec{c} x^5 + (\vec{d}' + e \vec{b}) x^7} $$

In this case, the two linear constraints coincide at $x^8$, the multiplicative constraint occurs at $x^{12}$, the ``error" terms $\ip{\vec{r}}{\vec{c}}$ and $\ip{\vec{m}}{\vec{d}' + e \vec{b}}$ coincide at $x^{10}$, and every other inner product can be simplified in public to a constant in the committed values.

The final family of equations can be transformed into an inner product in the same manner of Bulletproofs(+) by multiplying all the $r_i$ and $d_i$ values by a random power $q^{i+1}$. This transformation is equivalent to adjusting the basis for all the commitments since

$$ \com{V}{\vec{G}; \vec{v}} = \com{V}{\langle \frac{1}{q^{i+1}} G_i : i \rangle; \langle v_i q^{i+1} : i \rangle} $$

To recover the linear constraints, all the public vectors can be multiplied by the appropriate reciprocal power of $q$. Letting the diagonal matrix $Q_{ii} = q^{i+1}$, we can write all of these inner products using one vector polynomial equation in $x$

% $$ \ip{\frac{1}{x} Q^{-1} \vec{a} + x Q (e \vec{1} + \vec{d})}{\frac{1}{x} Q^{-1} \vec{b} + x Q \vec{r}}
%    - \ip{\frac{1}{x^2} Q^{-1} \vec{c}}{x^2 Q \vec{z}} = \frac{1}{x^2} f_0(q) + n + x^2 g_0(q) $$

% $$ f_0(q) = e \ip{\vec{1}}{\vec{b}} \qquad g_0(q) = \ip{Q^{-1} \vec{a}}{Q^{-1} \vec{b}} + |Q \vec{1}|^2 $$


$$ \ip{\frac{1}{x} Q (e \vec{1} + \vec{d}) + x Q^{-1} \vec{a}}{\frac{1}{x} Q^{-1} \vec{b} + x Q \vec{r}} - \ip{Q \vec{m}}{x^2 Q^{-1} \vec{c}} = \frac{1}{x^2} (n + f_0(q)) + g_0(q) $$

$$ f_0(q) = e \ip{\vec{1}}{\vec{b}} \qquad g_0(q) = \ip{Q^{-1} \vec{a}}{Q^{-1} \vec{b}} + \ip{Q \vec{1}}{Q \vec{1}} $$

The first terms of $f_0(q)$ and $g_0(q)$ encode inner products of constants, and the second term of $g_0(q)$ encodes the reciprocal constraints. The $x^2$ term vanishes since only if the second constraint holds, and the $x^{-2}$ term holds only if the first constraint holds. To convert this to an inner product of polynomials, the prover will then construct two polynomials in $y$

$$ \vec{l}(y) = \frac{1}{y} (\frac{1}{x} Q (e \vec{1} + \vec{d}) + x Q^{-1} \vec{a}) + y Q \vec{m} $$
$$ \vec{r}(y) = -\frac{1}{y} (x^2 Q^{-1} \vec{c}) + y (\frac{1}{x} Q^{-1} \vec{b} + x Q \vec{r}) $$
$$ \ip{\vec{l}(y)}{\vec{r}(y)} = \frac{1}{y^2} f_1(x, q) + (\frac{1}{x^2} (n + a_0(q)) + b_0(q)) + y^2 g_1(x,q) $$

Where $f_1$ and $g_1$ do not depend on $y$. Similarly, this inner product can be transformed to a norm of a polynomial in $z$

$$ | \frac{1}{z} \vec{l}(y) + z \vec{r}(y)|^2 = \frac{1}{z^2} a_1(y) + 2 \ip{\vec{l}(y)}{\vec{r}(y)} + z^2 b_1(y) $$

Where $f_2$ and $g_2$ similarly don't depend on $z$. Each stage of this process naturally transforms into a zero knowledge proof, with the $f_i$ and $g_i$ being responses to the corresponding challenge values $x, y, z$. Also, notice that each term involving commitments in all the equations is multiplied by a a distinct monomial in the challenges. This is the reason for the use of reciprocals, although the same effect can be achieved using $x$ and $x^3$. As in Bulletproofs+, $Q$ values can also be shifted up by $n+1$ so that the linear constraint will occur on the $q^{n+1}$ term, all the committed values on the $q^{2 i + 2}$ terms and the constraints on $q^i$. 

\section{Arithmetic Circuits}

Arithmetic circuits work very similarly to the Bulletproof(+) arithmetic circuit protocols. Given matrices of linear constraints such that left multiplication by $D$ is invertible, to check that $A \vec{x} + B \vec{y} + C \vec{z} = D \vec{v}$ and $x_i y_i = z_i$ we can reuse most of the range proof by just changing the initial polynomial slightly. First, we multiply the constraint matrices by a random vector from the left to transform matrix vector multiplications into inner produces. For $t_i = t^{i+1}$ let $\vec{a} = A^{\top} \vec{t}$, $\vec{b} = B^{\top} \vec{t}$, $\vec{c} = C^{\top} \vec{t}$, and $\vec{d} = D^{\top} \vec{t}$. We can then check

$$ \ip{\frac{1}{x} Q^{-1} \vec{b} + x Q \vec{x}}{\frac{1}{x} Q^{-1} \vec{a} + x Q \vec{y}}
   + \ip{\frac{1}{x^2} Q^{-1} \vec{c} - Q \vec{1}}{x^2 Q \vec{z}} = \frac{1}{x^2} f_0(q) + \ip{\vec{d}}{\vec{v}} $$

$$ f_0(q) = \ip{Q^{-1} \vec{b}}{Q^{-1} \vec{a}} $$

This polynomial changes some the terms in order to accommodate the different structure of the proof, but it is otherwise very similar to the range proof construction. The $y$ polynomial relation is also very similar, in as much as both the range proof and arithmetic circuits have left hand sides expressible as the sum of two inner products

$$ \vec{l}(y) = \frac{1}{y} (\frac{1}{x} Q^{-1} \vec{b} + x Q \vec{x}) + y x^2 Q \vec{z} $$
$$ \vec{r}(y) = \frac{1}{y} (\frac{1}{x^2} Q^{-1} \vec{c} - Q \vec{1}) + y (\frac{1}{x} Q^{-1} \vec{a} + x Q \vec{y}) $$
$$ \ip{\vec{l}(y)}{\vec{r}(y)} = \frac{1}{y^2} f_1(x, q) + (\frac{1}{x^2} f_0(q) + \ip{\vec{d}}{\vec{v}}) + y^2 g_1(x,q) $$

The norm is constructed in exactly the same way as the range proof case

$$ | \frac{1}{z} \vec{l}(y) + z \vec{r}(y)|^2 = \frac{1}{z^2} a_1(y) + 2 \ip{\vec{l}(y)}{\vec{r}(y)} + z^2 b_1(y) $$

As in the range proof, all the committed terms are multiplied by $Q$, although in this case one of the constants is also multiplied by $Q$ to account for the $x_i y_i = z_i$ having a quadratic equal to linear term. The commitments are also each multiplied by a unique monomial in the challenge values, which ensures the soundness of the final linear check. For a proof over vectors of length $n$, the arithmetic circuit proof costs $2 \rceil \log_2 n \lceil + 5$ group elements and $3$ scalars, which is the same as the range proof, and does not count the value commitments.   

\section{Proofs} 

Bulletproofs, Bulletproofs+, and many other zero knowledge proofs fall under the framework of an iterated sigma protocol. That is, a protocol composed of rounds where the prover provides some commitments, the verifier selects some challenges, and then the prover either repeats this or provides the opening of a linear combination of the commitments that varies according to the challenges. Building upon the general forking lemma [], I attempt to give general proofs of Completeness/Soundness, Special Honest Verifier Zero Knowledge, and Witness Extraction for a general class of protocols and then show that Bulletproofs- is one such protocol.

To do this, we will place several constraints on the behavior of the prover. Firstly, all responses of the prover must be uniformly distributed at random over either the elliptic curve group or the finite field. This simplifies the proof of SHVZK, and it can be accomplished simply by requiring every commitment to be blinded and every scalar opening to be a linear combination of at least one blinding value. 

The more subtle requirement deals with generic Witness Extraction. The basic idea is to take the final stage of the proof, where the prover provides openings of some linear combination of the commitments, and re-run the final oracle request to extract an opening of the responses from the previous round. Substituting this into the original opening, we obtain a new set of openings that are independent of the final responses from the prover and run the same technique again on these new found opening with the second to last challenge and so on.

In order for this to work, the prover must actually use the challenges in a manner that allows extracting linearly independent equations in the responses with sufficiently high probability. This limits how the prover can use challenges in an absolute sense, but in fact is more or less equivalent to the proof itself being sound. As a generalization of the Schwartz-Zippel lemma, we can constrain the kind of linear constraints the prover may use to only those where the probability of a sampling of $n$ vectors of length greater than or equal to $n$ having rank less than $n$ is negligible. Examples of these sorts of vectors include: vectors with entries sampled uniformly at random, vectors sampled from a monomial basis, and vectors sampled from $\frac{1}{e + r_i}$ for fixed $r_i$ and random $e$.

Given the final linear constraints, we then require that taken as vectors over functions of subsequent challenges, this property is satisfied by the induced distribution of the linear constraints. For example, if we take the linear combination $A + x y B + x y^2 C$ this is a vector over both the responses $A, B, C$ and the $y$ linear combinations of responses $\{A, y B + y^2 C\}$. In both cases, vectors $\langle 1, x y, x y^2 \rangle$ and $\langle 1, x \rangle$ for uniformly distributed $x, y$ have degenerate rank with negligible probability. Note, this does restrict certain kinds of monomial bases, for example if the challenges $x, y, z$ are drawn in that order, then 

While this might seem like a peculiar constraint, it is more or less equivalent to the soundness of the underlying protocol. If it were not the case that different challenge vectors were linearly independent, the resulting linear equations could just be simplified algebraically. 


\newpage



 we must first construct a suitable quadratic equation. To do this, observe that the prover must show that $\sum_{i=0}^{k-1} d_i b^i = n$, $\sum_{i=0}^{k-1} r_i = \sum_{j=0}^{d-1} \frac{m_j}{e + j}$, and $d_i r_i = 1$. For convenience, assume that $p_i = \frac{1}{e + i}$ if $0 \leq i < d$, is zero elsewhere, and that $m_j$ is similarly extended so all the vectors are the same length. We can then arrange the equations into one polynomial equation

$$ \sum_{i=0}^{k-1} (\frac{(e + d_i) q^i}{x} + \frac{x}{q^i}) (\frac{b^i}{x q^i} + r_i q^i x) - (m_i q^i) (p_i / q^i) x^2 = \frac{n + f_0(q)}{x^2} + g_0(q) $$

Where $f_0(q)$ and $g_0(q)$ do not depend on committed values only if the original equations are satisfied. In the above equation, all the secret or committed values are multiplied by $q^i$ and the public values by $\frac{1}{q^i}$. This equation itself can be transformed into an inner product of polynomials as 

$$ l_i(y) = \frac{1}{y} (\frac{(e + d_i) q^i}{x} + \frac{x}{q^i}) + y m_i q^i $$
$$ r_i(y) = \frac{1}{y} (\frac{- p_i x^2}{q^i}) + y (\frac{b^i}{x q^i} + r_i q^i x) $$
$$ \ip{\vec{l}(y)}{\vec{r}(y)} = a_0(x, q) \frac{1}{y^2} + (f_0(q) + n + g_0(q) x) + b_0(x,q) y^2 $$

Which can similarly be transformed into a norm of polynomials by taking

$$ |\vec{l}(y) \frac{1}{z} + \vec{r}(y) z|^2 = a_1(y) \frac{1}{z^2} + 2 \ip{\vec{l}(y)}{\vec{r}(y)} + b_1(y) z^2 $$

The process of constructing this norm equality naturally has the challenge/response structure of a zero knowledge proof, where each of the coefficients from each round is sent as a response to the previous challenge. Note also that the commitment to $r_i$ is selected after the commitments to $d_i$ and $m_i$ are sent. Putting this all together into one proof 

\begin{enumerate}
	\item Commit $N = \com{V}{\vec{n}}, D = \com{V}{\vec{d}}, M = \com{V}{\vec{m}}$
	\item Select $e \leftarrow \mathbb{F}_p$
	\item Commit $R = \com{V}{\vec{r}}$
	\item Select $x, q \leftarrow \mathbb{F}_p$
	\item Commit $A_0 = \com{S}{a_0(x, q)}, B_0 = \com{S}{b_0(x, q)}$
	\item Select $y \leftarrow \mathbb{F}_p$
	\item Commit $A_1 = \com{S}{a_1(y)}, B_1 = \com{S}{b_1(y)}$
	\item Select $z \leftarrow \mathbb{F}_p$
	\item Norm Proof on $P = \frac{1}{z^2} A_1 + z^2 B_1 + \frac{1}{y^2} A_0 + y^2 B_0 + \frac{1}{x^2} N + \frac{1}{x y z} D + x y z R + \frac{y}{z} M + \com{V}{\vec{V}}$
\end{enumerate}

Where $V$ is a publicly computable vector of all the terms that depend only on constants and challenges. Taking the commitment $N$ as input to the protocol, and therefore not counting it in the cost, these steps require transmitting $7$ group elements to the verifier, and the norm proof will require sending another $2 \lceil \log_2(k) \rceil$ group elements and $3$ scalars. In all, for a $64$ bit value this costs $15$ group elements and $3$ scalars, the same as a $64$ bit Bulletproof+, versus $16$ and $5$ for a Bulletproof.

Like Bulletproofs, this protocol naturally extends to multiple values. To do so, we can substitute $N$ for a list of value commitments $N_0..N_l$, $\vec{d}$ for the concatenated list of all the digits, $\vec{r}$ the concatenated list of all the reciprocals, and $\vec{m}$ for the \textit{sum} of the individual digit counts, or equivalently the total digit count. Everything works exactly the same except for the linear constraint. Reusing the $e$ challenge we can instead check

$$ \sum_{j=1}^l e^j N_j = \sum_{j=1}^l \sum_{i=0}^{k - 1} d_i b^i e^j $$

\subsection{Verification}

While this technique does have an moderate advantage over Bulletproofs in terms of proof size, and parity with Bulletproofs+, for $64$ bit range proofs, the real advantage of this protocol is in the proving and verification time. For both the verifier and the prover, the significant majority of their time is spent computing a number of elliptic curve scalar-point multiplications proportional to the length of the vector commitments. For both Bulletproofs(+) protocols, these vectors have $128$ elements, whereas in Bulletproofs- the vectors are of length $16$, eight times shorter. This ratio holds for aggregated proofs as well.

The verification of a proof can be structured as a sequence of hash function evaluations followed by a single multiexponentiation check. For an $m$ bit range proof, 

\subsection{Secure Multiparty Computation}

There is a complication with secure multiparty computation, since the $m_i$ values are not known to any of the individual participants, and must remain secret. 

\fi


\end{document}